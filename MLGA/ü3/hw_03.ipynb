{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Neural Network Classifier from Scratch (14p.)\n",
    "\n",
    "In this exercise, we will implement a small neural network from scratch, i.e., only using numpy. This is nothing you would do \"in real life\" but it is a good exercise to deepen understanding. \n",
    "\n",
    "The network will consist of an arbitrary number of hidden layers with ReLU activation, a sigmoid output layer (as we are doing binary classification) and we will train it using the binary cross-entropy (negative Bernoulli likelihood) loss function. \n",
    "\n",
    "Ok, so let's start by importing and loading what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "# load our two moons (I promise we will get a new dataset in the next exercise)\n",
    "two_moons_train_data = dict(np.load(\"two_moons.npz\", allow_pickle=True)) \n",
    "two_moons_test_data = dict(np.load(\"two_moons_test.npz\", allow_pickle=True))\n",
    "# we need to reshape our labels so that they are [N, 1] and not [N] anymore\n",
    "two_moons_train_samples = two_moons_train_data[\"samples\"]\n",
    "two_moons_train_labels = two_moons_train_data[\"labels\"][:, None]\n",
    "two_moons_test_samples = two_moons_test_data[\"samples\"]\n",
    "two_moons_test_labels = two_moons_test_data[\"labels\"][:, None]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.) Auxillary Functions (3 p.)\n",
    "We start with implementing some auxiliary functions we are going to need later. The sigmoid and ReLU activation functions, the binary cross-entropy loss as well as their derivatives.\n",
    "\n",
    "The binary cross entropy loss is given as \n",
    "$ - \\dfrac{1}{N} \\sum_{i=1}^N (y_i \\log (p_i) + (1 - y_i) \\log (1 - p_i)) $ where $y_i$ denotes the ground truth label and $p_i$ the network prediction for sample $i$.\n",
    "\n",
    "**Hint** All derivatives were derived/implemented during the lecture or previous exercise - so feel free to borrow them from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    elementwise relu activation function\n",
    "    :param x: input to function [shape: arbitrary]\n",
    "    :return : relu(x) [shape: same as x]\n",
    "    \"\"\"\n",
    "    ### TODO #########################\n",
    "    return np.maximum(0, x)\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def d_relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    elementwise gradient of relu activation function\n",
    "    :param x: input to function [shape: arbitrary]\n",
    "    :return : d relu(x) / dx [shape: same as x]\n",
    "    \"\"\"\n",
    "    ### TODO #########################\n",
    "    return (x > 0).astype(float)\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    elementwise sigmoid activation function\n",
    "    :param x: input to function [shape: arbitrary]\n",
    "    :return : sigmoid(x) [shape: same as x]\n",
    "    \"\"\"\n",
    "    ### TODO #########################\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def d_sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    elementwise sigmoid activation function\n",
    "    :param x: input to function [shape: arbitrary]\n",
    "    :return : d sigmoid(x) /dx [shape: same as x]\n",
    "    \"\"\"\n",
    "    ### TODO #########################\n",
    "    sigmoid_x = sigmoid(x)\n",
    "    return sigmoid_x * (1 - sigmoid_x)\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def binary_cross_entropy(predictions: np.ndarray, labels: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    binary cross entropy loss (negative bernoulli ll)\n",
    "    :param predictions: predictions by model [shape [N]]\n",
    "    :param labels: class labels corresponding to train samples, [shape: [N]]\n",
    "    :return binary cross entropy\n",
    "    \"\"\"\n",
    "    ### TODO #########################\n",
    "    epsilon = 1e-7  # small constant to avoid division by zero\n",
    "    loss = -np.mean(labels * np.log(predictions + epsilon) + (1 - labels) * np.log(1 - predictions + epsilon))\n",
    "    return loss\n",
    "    ##################################\n",
    "\n",
    "\n",
    "def d_binary_cross_entropy(predictions: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    gradient of the binary cross entropy loss\n",
    "    :param predictions: predictions by model [shape [N]]\n",
    "    :param labels: class labels corresponding to train samples, [shape [N]]\n",
    "    :return gradient of binary cross entropy, w.r.t. the predictions [shape [N]]\n",
    "    \"\"\"\n",
    "    ### TODO #########################\n",
    "    epsilon = 1e-7  # small constant to avoid division by zero\n",
    "    grad = - (labels / (predictions + epsilon)) + ((1 - labels) / (1 - predictions + epsilon))\n",
    "    return grad\n",
    "    ##################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Setting up the Network and Initialization (2 p.)\n",
    "\n",
    "Next, we are going to set up the neural network. We will represent it as a list of weight matrices and a list of bias vectors. Each list has one entry for each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(neurons_per_hidden_layer: List[int], \n",
    "                 input_dim: int, \n",
    "                 output_dim: int) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    :param neurons_per_hidden_layer: list of numbers, indicating the number of neurons of each hidden layer\n",
    "    :param input_dim: input dimension of the network\n",
    "    :param output_dim: output dimension of the network\n",
    "    :param seed: seed for random number generator\n",
    "    :return list of weights and biases as specified by dimensions and hidden layer specification\n",
    "    \"\"\"\n",
    "    # seed random number generator\n",
    "    current_input_dim = input_dim\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    # initialize weight and bias for each hidden layer and the output layer\n",
    "    for current_output_dim in neurons_per_hidden_layer + [output_dim]:\n",
    "        \n",
    "        #### TODO ##################\n",
    "        # Initialize the bias with zeros and the weights with the relu corrected Xavier activation \n",
    "        # You can use np.random.normal to sample from N(0, 1).\n",
    "        bias = np.zeros(current_output_dim)\n",
    "        limit = np.sqrt(2 / (current_input_dim + current_output_dim))\n",
    "        weight = np.random.normal(0, limit, (current_input_dim, current_output_dim))\n",
    "\n",
    "        ############################\n",
    "        # append new weight and bias\n",
    "        weights.append(weight)\n",
    "        biases.append(bias)\n",
    "        # next layer has current output dim as input dim \n",
    "        current_input_dim = current_output_dim\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Forward Pass (3 p.)\n",
    "\n",
    "The next step is the forward pass, i.e., propagating a batch of samples through the network to get the final prediction.\n",
    "But that's not all - to compute the gradients later we also need to store all necessary quantities, here those are:\n",
    "- The input to every layer (here called h's)\n",
    "- The \"pre-activation\" of every layer, i.e., the quantity that is fed into the non-linearity (here called z's)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x: np.ndarray, \n",
    "                 weights: List[np.ndarray],\n",
    "                 biases: List[np.ndarray]) -> Tuple[np.ndarray, List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    propagate input through network\n",
    "    :param x: input: (shape, [N x input_dim])\n",
    "    :param weights: weight parameters of the layers\n",
    "    :param biases: bias parameters of the layers\n",
    "    :return: - Predictions of the network [shape, [N x out_put_dim]]\n",
    "             - hs: output of each layer [input + all hidden layers] [length: len(weights)]\n",
    "             - zs: preactivation of each layer [all hidden layers + output] [length: len(weights)]\n",
    "    \"\"\"\n",
    "\n",
    "    hs = []  # list to store all inputs\n",
    "    zs = []  # list to store all pre-activations\n",
    "    \n",
    "    # input to first hidden layer is just the input to the network \n",
    "    h = x\n",
    "    hs.append(h)\n",
    "    \n",
    "    ### TODO #########################\n",
    "    # pass trough hidden layers\n",
    "    for i in range(len(weights) - 1):\n",
    "        z = h @ weights[i] + biases[i]\n",
    "        zs.append(z)\n",
    "        h = sigmoid(z)\n",
    "        hs.append(h)\n",
    "    ##################################\n",
    "\n",
    "    # output layer\n",
    "    z = h @ weights[-1] + biases[-1]  \n",
    "    zs.append(z) # store last pre-activation \n",
    "    y = sigmoid(z)\n",
    "\n",
    "    return y, hs, zs\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Backward Pass (4 p.)\n",
    "\n",
    "For training by gradient descent we need - well - gradients. Those are computed using backpropagation during the so-called \"backward pass\". We will use the chain rule to propagate the gradient back through the network and, at every layer, compute the gradients for the weights and biases at that layer. The initial gradient is given by the gradient of the loss function w.r.t. the network output.\n",
    "\n",
    "**Hint** recall that you can implement a bached outer product of two batches of vectors, `a` (with shape `(n, x)`) and `b` (with shape `(n,y)`) as \n",
    "\n",
    "`np.expand_dims(a, axis=1) * np.expand_dims(b, axis=2)`\n",
    "\n",
    "(will have shape `(n, y, x)`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(loss_grad: np.ndarray, \n",
    "                  hs: List[np.ndarray], \n",
    "                  zs: List[np.ndarray], \n",
    "                  weights: List[np.ndarray],\n",
    "                  biases: List[np.ndarray]) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    propagate gradient backwards through network\n",
    "    :param loss_grad: gradient of the loss function w.r.t. the network output [shape: [N, 1]]\n",
    "    :param hs: values of all hidden layers during forward pass\n",
    "    :param zs: values of all preactivations during forward pass\n",
    "    :param weights: weight paramameters of the layers\n",
    "    :param biases: bias parameters of the layers\n",
    "    :return: d_weights: List of weight gradients - one entry with same shape for each entry of \"weights\"\n",
    "             d_biases: List of bias gradients - one entry with same shape for each entry of \"biases\"\n",
    "    \"\"\"\n",
    "\n",
    "    # return gradients as lists - we pre-initialize the lists as we iterate backwards\n",
    "    d_weights = [None] * len(weights)\n",
    "    d_biases = [None] * len(biases)\n",
    "\n",
    "    ### TODO #########################\n",
    "    d_z = loss_grad * sigmoid(zs[-1]) * (1 - sigmoid(zs[-1]))\n",
    "\n",
    "    for i in range(len(weights) - 1, -1, -1):\n",
    "        if i > 0:\n",
    "            d_h = np.matmul(weights[i], d_z.T).T\n",
    "            d_z = d_h * sigmoid(zs[i-1]) * (1 - sigmoid(zs[i-1]))\n",
    "        d_weights[i] = np.expand_dims(hs[i], axis=2) * np.expand_dims(d_z, axis=1)\n",
    "        d_biases[i] = d_z\n",
    "\n",
    "    ##################################\n",
    "\n",
    "    return d_weights, d_biases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5) Optimization (2 p.)\n",
    "\n",
    "Finally, we are going to implement a training procedure for our neural network. We first initialize the network using the previously implemented method and then optimize it using stochastic gradient descent. For this, we extend our stochastic gradient descent implementation from the first exercise with a simple momentum term $m$, which forms a running average over the gradient $g$, i.e.,\\n\n",
    "$$\\boldsymbol{m}_{k+1} = \\gamma \\boldsymbol{m}_k + (1 - \\gamma) \\boldsymbol{g}_k$$\n",
    "where $\\gamma$ denotes the constant momentum factor. We will then use $m$ instead of $g$ for the parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_net(train_samples: np.ndarray,\n",
    "                     train_labels: np.ndarray,\n",
    "                     test_samples: np.ndarray,\n",
    "                     test_labels: np.ndarray,\n",
    "                     layers: List[int],\n",
    "                     learning_rate: float,\n",
    "                     momentum_factor: float, \n",
    "                     num_iterations: int,\n",
    "                     batch_size: int) -> Tuple[List[np.ndarray], List[np.ndarray], np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    initialize and train the neural network, also keeps track of train and test error during training.\n",
    "    :param train_samples: samples to train on [shape [N, sample_dim]]\n",
    "    :param train_labels: labels corresponding to train samples, [shape [N, 1]]\n",
    "    :param test_samples: samples to test on [shape [M, sample_dim]]\n",
    "    :param test_labels: labels corresponding to test samples, [shape [M, 1]]\n",
    "    :param layers: list of integers, indicating sizes of hidden layers\n",
    "    :param learing_rate: size of optimizer update steps \n",
    "    :param momentum_factor: factor gamma to control the momentum moving average \n",
    "    :param num_iterations: total number of iterations (i.e. passes over whole dataset)\n",
    "    :param batch_size: size of batches, has to evenly devide N, 1 for full stochastic gradient descent,\n",
    "                       N for full batch gradient descent\n",
    "    \n",
    "    :return: list of trained weights, \n",
    "             list of trained biases, \n",
    "             train errors during training,\n",
    "             test errors during training\n",
    "    \"\"\"\n",
    "    \n",
    "    assert 1 <= batch_size <= train_samples.shape[0]\n",
    "    # This is a somewhat simplifying assumption but for the exercise its ok\n",
    "    assert train_samples.shape[0] % batch_size == 0, \"Batch Size does not evenly divide number of samples\"\n",
    "    batches_per_iter = int(train_samples.shape[0] / batch_size)\n",
    "\n",
    "    \n",
    "    # init model\n",
    "    weights, biases = init_weights(layers, \n",
    "                                   input_dim=train_samples.shape[-1], \n",
    "                                   output_dim=train_labels.shape[-1])\n",
    "\n",
    "    #book keeping\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    momentum_weights = [np.zeros(w.shape) for w in weights]\n",
    "    momentum_biases = [np.zeros(b.shape) for b in biases]\n",
    "    for i in range(num_iterations):\n",
    "        rnd_idx = np.random.permutation(train_samples.shape[0])\n",
    "        for j in range(batches_per_iter):\n",
    "\n",
    "            global_idx = i * batches_per_iter + j\n",
    "            \n",
    "            sample_batch = train_samples[rnd_idx[j * batch_size: (j + 1) * batch_size]]\n",
    "            label_batch = train_labels[rnd_idx[j * batch_size: (j + 1) * batch_size]]\n",
    "        \n",
    "            # predict network outputs and record intermediate quantities using the forward pass\n",
    "            prediction, hs, zs = forward_pass(sample_batch, weights, biases)\n",
    "            train_losses.append(binary_cross_entropy(prediction, label_batch))\n",
    "\n",
    "            # compute gradients\n",
    "            loss_grad = d_binary_cross_entropy(prediction, label_batch)\n",
    "            w_grads, b_grads = backward_pass(loss_grad, hs, zs, weights, biases)\n",
    "            \n",
    "            ##### TODO ####\n",
    "            # update momentum terms and parameters using the previously computed gradients. \n",
    "            # Todo:\n",
    "            ###############\n",
    "            test_losses.append(binary_cross_entropy(forward_pass(test_samples, weights, biases)[0], test_labels))\n",
    "        \n",
    "    return weights, biases, train_losses, test_losses "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tying Everything Together \n",
    "\n",
    "Finally, we can tie everything together and train our network. Train and test accuarcy should be larger than 0.95. If the results do not seem to have converged yet, try increasing the num_iterations argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed random number generator\n",
    "np.random.seed(42)\n",
    "\n",
    "# train network\n",
    "weights, biases, train_losses, test_losses = train_neural_net(train_samples=two_moons_train_samples,\n",
    "                                                              train_labels=two_moons_train_labels,\n",
    "                                                              test_samples=two_moons_test_samples,\n",
    "                                                              test_labels=two_moons_test_labels,\n",
    "                                                              learning_rate=5e-2,\n",
    "                                                              num_iterations=25,\n",
    "                                                              momentum_factor=0.99,\n",
    "                                                              batch_size=1,\n",
    "                                                              layers=[64, 64])\n",
    "\n",
    "# plotting\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(train_losses)\n",
    "plt.plot(test_losses)\n",
    "plt.xlabel(\"Gradient Steps\")\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.grid(\"on\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "\n",
    "\n",
    "# evaluate accuracy \n",
    "train_predictions = forward_pass(two_moons_train_samples, weights, biases)[0]\n",
    "test_predictions = forward_pass(two_moons_test_samples, weights, biases)[0]\n",
    "\n",
    "predicted_train_labels = np.ones(train_predictions.shape)\n",
    "predicted_train_labels[train_predictions < 0.5] = 0\n",
    "print(\"Train Accuracy: \", \n",
    "      np.count_nonzero(predicted_train_labels == two_moons_train_labels) / len(two_moons_train_samples))\n",
    "\n",
    "predicted_test_labels = np.ones(test_predictions.shape)\n",
    "predicted_test_labels[test_predictions < 0.5] = 0\n",
    "print(\"Test Accuracy: \",\n",
    "      np.count_nonzero(predicted_test_labels == two_moons_test_labels) / len(two_moons_test_samples))\n",
    "\n",
    "\n",
    "def plt_solution(samples, labels):\n",
    "    plt_range = np.arange(-1.5, 2.5, 0.01)\n",
    "    plt_grid = np.stack(np.meshgrid(plt_range, plt_range), axis=-1)\n",
    "    plt_grid_shape = plt_grid.shape[:2]\n",
    "    pred_grid = np.reshape(forward_pass(plt_grid, weights, biases)[0], plt_grid_shape)\n",
    "    plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[0.5], colors=[\"black\"])\n",
    "    plt.contourf(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=10)\n",
    "    plt.colorbar()\n",
    "    s0 = plt.scatter(x=samples[labels[:, 0] == 0, 0], y=samples[labels[:, 0] == 0, 1],\n",
    "                     label=\"c=0\", c=\"blue\")\n",
    "    s1 = plt.scatter(x=samples[labels[:, 0] == 1, 0], y=samples[labels[:, 0] == 1, 1],\n",
    "                     label=\"c=1\", c=\"orange\")\n",
    "    plt.legend([s0, s1], [\"c0\", \"c1\"])\n",
    "    plt.xlim(-1.5, 2.5)\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Trained Network - with train samples\")\n",
    "plt_solution(two_moons_train_samples, two_moons_train_labels)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Trained Network - with test samples\")\n",
    "plt_solution(two_moons_test_samples, two_moons_test_labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) MNIST Classifier with PyTorch (6 p.)\n",
    "\n",
    "Modern deep learning approaches are mostly implemented using special libraries, providing functionality such as automatic differentiation, common SGD Optimiziers, easy usage of GPUs and so on. We will use PyTorch, at the moment the, arguably, most common framework (for research).\n",
    "\n",
    "## Getting Started\n",
    "You can find a documentation of the PyTorch API here https://pytorch.org/docs/stable/torch.html# . Don't worry if it seems a lot, we will point out the relevant bits during the exercise as we go along, also feel free to take another look at the tutorial notebook, some details are introduced there in more depth.\n",
    "\n",
    "**Installation** \n",
    "You can find installation instructions here https://pytorch.org/ . Take the most recent stable version (1.10.X). We won't use GPUs here so you can take the cuda-free installation. We also don't need torchvision nor torchaudio so those don't need to be installed.\n",
    "\n",
    "**Data**\n",
    "We finally use a new dataset. The classical MNIST Handwritten Digit Classification set. It consists of grayscale images of size 28x28 of handwritten digits. Let's load it and visualize some of the images. We also do some preprocessing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "data_dict = dict(np.load(\"mnist.npz\"))\n",
    "\n",
    "# prepare data:\n",
    "# - Images are casted to float 32 (from uint8) mapped in interval (0,1) and a \"fake\" color channel is added.\n",
    "#   torch uses \"NCHW\"-layout for 2d convolutions. (i.e., a batch of images is represented as a 4 d tensor \n",
    "#   where the first axis (N) is the batch dimension, the second the (color) **C**hannels, followed by a **H**eight\n",
    "#   and a **W**idth axis). As we have grayscale images there is only 1 color channel.  \n",
    "# - targets are mapped to one hot encoding - torch does that for us \n",
    "with torch.inference_mode():\n",
    "    train_samples = torch.from_numpy(data_dict[\"train_samples\"].astype(np.float32) / 255.0).reshape(-1, 1, 28, 28)\n",
    "    train_labels = torch.nn.functional.one_hot(torch.from_numpy(data_dict[\"train_labels\"])).float()\n",
    "    test_samples = torch.from_numpy(data_dict[\"test_samples\"].astype(np.float32) / 255.0).reshape(-1, 1, 28, 28)\n",
    "    test_labels = torch.nn.functional.one_hot(torch.from_numpy(data_dict[\"test_labels\"])).float()\n",
    "\n",
    "\n",
    "# plot first 25 images in train set\n",
    "plt.figure(figsize=(25, 1))\n",
    "for i in range(25):\n",
    "    plt.subplot(1, 25, i + 1)\n",
    "    # drop channel axis for plotting\n",
    "    plt.imshow(train_samples[i, 0], cmap=\"gray\", interpolation=\"none\")\n",
    "    plt.gca().axis(\"off\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Specifiying the Network (6 p.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task here is to build a convolutional classifier for the MNIST Classification task. This classifier should consist of a convolutional part, followed by a fully connected part, see the general structure below, and also see the tutorial notebook for further details.\n",
    "\n",
    "Designing the explicit architecture is up to you, but your classifier should reach a test set accuracy of at least 95% with the hyperparameters specified below. \n",
    "\n",
    "In the torch API under torch.nn you can find everything you need. Consider using some of the following classes, (you don't have to use all, you can use one multiple times) \n",
    "\n",
    "**Layers**\n",
    "- Linear,\n",
    "- Conv2d,\n",
    "\n",
    "**Pooling** \n",
    "- MaxPool2d\n",
    "- AvgPool2d \n",
    "\n",
    "**Activations**\n",
    "- ReLU\n",
    "- ELU\n",
    "- Tanh\n",
    "- Sigmoid\n",
    "- ... (there are many more)\n",
    "\n",
    "**Write a few sentences about how you proceeded and what you observed while trying different architectures!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = torch.nn\n",
    "\n",
    "def get_conv_net() -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        # input are images of shape 1 (color) x 28 (heigth) x 28 (width)\n",
    "        ## TODO ##########\n",
    "        # add some conv layers and pooling\n",
    "        ##################\n",
    "        nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        \n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        # At the transition between the conv layers and the dense layers we need to \"flatten\" the data, \n",
    "        # i.e. transform from a 4 d tensor (batch x dims of last set of filters) to a 2 d tensor \n",
    "        # ( batch x product(dims of last set of filters) )\n",
    "        nn.Flatten(),\n",
    "        ## TODO ##########\n",
    "        # add some fully connected layers\n",
    "        ##################\n",
    "        nn.Linear(in_features=64 * 5 * 5, out_features=128),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        # the last layer should have output size 10 (one entry for each class))\n",
    "        nn.Linear(in_features=128, out_features=train_labels.shape[-1]),\n",
    "        # the activation is \"implict\", i.e., we do not need to add a softmax layer. Instead \n",
    "        # the loss is computed directly using the \"logits\" (i.e. the values that are input to the softmax)\n",
    "        # by nn.functional.cross_entropy later. This is numerically much more stable, we only need to remember to \n",
    "        # add the softmax when prediction class labels for evaluation.\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We that we have specified the network we can train it using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "num_iters = 5  # small number of epochs should be sufficient to get descent performance\n",
    "batch_size = 64\n",
    "\n",
    "# making sure this cell behaves deterministically \n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "classifier = get_conv_net().to(torch.device(\"cpu\"))\n",
    " \n",
    "# Train data set and loader, see recap notebook (or pytorch documentation for more detail on this)\n",
    "train_set = torch.utils.data.TensorDataset(train_samples, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Optimizer \n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3) \n",
    "\n",
    "\n",
    "# train loop\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for i in range(num_iters):\n",
    "    print(\"Iteration {:03d}\".format(i + 1))\n",
    "    for batch in train_loader:\n",
    "        # forward pass\n",
    "        samples, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = classifier(samples)   # predict logits\n",
    "        loss = nn.functional.cross_entropy(input=predictions,\n",
    "                                           target=labels)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update step \n",
    "        optimizer.step()\n",
    "        #########\n",
    "        train_losses.append(loss.detach().numpy())\n",
    "    \n",
    "    with torch.inference_mode():      \n",
    "        predictions = classifier(test_samples)\n",
    "        loss = nn.functional.cross_entropy(input=predictions,\n",
    "                                           target=test_labels)\n",
    "        # don't forget to add the softmax when predicting the class labels ;) \n",
    "        predicted_labels = nn.functional.softmax(predictions, dim=-1).argmax(dim=-1)\n",
    "        test_losses.append(loss.numpy())\n",
    "        acc = torch.count_nonzero(predicted_labels == test_labels.argmax(dim=-1)) / test_labels.shape[0]\n",
    "        print(\"Test Set Accuracy: {:.3f}, Test Loss {:.3f}\".format(acc.numpy(), loss.numpy()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.figure()\n",
    "num_train_batches = len(train_loader)\n",
    "plt.semilogy(train_losses, label=\"train loss\")\n",
    "plt.semilogy(np.arange(num_train_batches, num_iters * num_train_batches+1, num_train_batches),\n",
    "             test_losses,\n",
    "             label=\"test_loss\")\n",
    "plt.xlabel(\"gradient steps\")\n",
    "plt.ylabel(\"cross entropy loss\")\n",
    "plt.grid(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
