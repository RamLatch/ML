{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: ML - Grundlagen und Algorithm, 28 Pts\n",
    "\n",
    "Extended Deadline: July 19 15:00"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.) Reminders\n",
    "Please adhere to the hand-in conventions specified in the 0-th exercise sheet, i.p.,\n",
    "- You have to **submit the jupyter notebook file as well as the PDF**!\n",
    "- Please **adhere to the zip naming conventions**!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.) Rejection Sampling (6 Points)\n",
    "In this exercise you will implement rejection sampling step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let us import some libraries. Please note that we are using PyTorch again.\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "The goal of rejection sampling is to sample from a target density $x \\sim p(x)$ only having access to the un-normalized target density $ = \\tilde{p}(x) = Zp(x)$ with $Z=\\int \\tilde{p}(x) dx$.\n",
    "\n",
    "### Target Density $\\tilde{p}(x)$\n",
    " Let's define the target distribution. In this case it is a Gaussian mixture model (GMM). Please note that it is straightforward to sample from a GMM. However, for demonstration purposes, we assume that it is not possible and we use rejection sampling to obtain samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetDist:\n",
    "    def __init__(self, means=[-1.5, 1.5, 2.5], stds=[0.4, 0.3, 0.5]):\n",
    "        \"\"\"\n",
    "        :param means: Means for the Gaussian mixture components\n",
    "        :param stds: Standard deviations for the Gaussian mixture components\n",
    "        \"\"\"\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "\n",
    "    def probs(self, samples):\n",
    "        \"\"\"\n",
    "        :param samples: Samples we want to evaluate ̃p(x)\n",
    "        :return: ̃p(x)\n",
    "        \"\"\"\n",
    "        gaussian_log_probs = [Normal(loc=self.means[i], scale=self.stds[i]).log_prob(samples) for i in range(len(self.means))]\n",
    "        return torch.exp(torch.logsumexp(torch.cat(gaussian_log_probs,-1), -1) - math.log(len(self.means)))\n",
    "\n",
    "# Let us plot the target density\n",
    "x = torch.linspace(-5, 5, 100).reshape(-1,1)\n",
    "plt.plot(x, TargetDist().probs(x))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Proposal Distribution $q(x)$ (2 Points)\n",
    "Next, we need to define a proposal distribution $q(x)$ where we can easily sample from. In this exercise we use a Gaussian distribution $q(x) = \\mathcal{N}(\\mu, \\sigma^2)$. For the proposal distribution we need to operations:\n",
    "- We need to be able to evaluate the likelihood $q(x)$\n",
    "- We need to be able to sample from $q(x)$\n",
    "\n",
    "Implement these operations in the following template class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProposalDist:\n",
    "    def __init__(self, mean=0., std=1.5):\n",
    "        \"\"\"\n",
    "        :param mean: Mean of the Gaussian proposal distribution\n",
    "        :param std: Standard deviation of the Gaussian proposal distribution\n",
    "        \"\"\"\n",
    "        self.means = mean\n",
    "        self.stds = std\n",
    "        self.proposal_dist = Normal(mean, std)\n",
    "\n",
    "\n",
    "    def probs(self, samples):\n",
    "        \"\"\"\n",
    "        :param samples: Samples x to evaluate q(x)\n",
    "        :return: q(x)\n",
    "        \"\"\"\n",
    "        ############################################################\n",
    "        # TODO Compute q(x)\n",
    "        ############################################################\n",
    "        return self.proposal_dist.log_prob(samples).exp()\n",
    "\n",
    "\n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"\n",
    "        :param n_samples: Number of samples to draw from q(x)\n",
    "        :return: x ~ q(x)\n",
    "        \"\"\"\n",
    "        ############################################################\n",
    "        # TODO Draw samples x ~ q(x)\n",
    "        ############################################################\n",
    "        return self.proposal_dist.sample((n_samples,))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to find a $c$ such that $\\forall x \\in \\mathbb{R}: cq(x) \\geq \\tilde{p}(x)$. Since, the target distribution only has significant probability mass in $I = [-5,5]$ we are looking for a $c$ that full-fills the condition on $I$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = torch.linspace(-5,5, 1000).reshape(-1,1)\n",
    "q = GaussianProposalDist()\n",
    "p_tilde = TargetDist()\n",
    "\n",
    "q_x = q.probs(I)\n",
    "p_tilde_x = p_tilde.probs(I)\n",
    "############################################################\n",
    "# TODO Compute c such that it full-fills the condition above\n",
    "############################################################\n",
    "c = (p_tilde_x / q_x).max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do a quick sanity check by visualizing the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(I, p_tilde_x, label='$\\\\tilde{p}(x)$')\n",
    "plt.plot(I, c * q_x, label='$q(x)$')\n",
    "plt.xlabel('$x$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Sampling (4 Points)\n",
    "\n",
    "Now we have everything to perform rejection sampling which involves the following three steps:\n",
    "1. Draw samples $s \\sim q(x)$\n",
    "2. Draw samples $u \\sim \\text{Uniform}[0, cq(s)]$\n",
    "3. Reject $s$ if $u > \\tilde{p}(s)$ else accept\n",
    "\n",
    "In the following you are tasked to implement these three steps. Save the accepted as well as the rejected samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 500\n",
    "############################################################\n",
    "# TODO 1. Draw samples s ~ q(x)\n",
    "############################################################\n",
    "proposal_samples = q.sample(N_SAMPLES)\n",
    "############################################################\n",
    "# TODO 2. Draw samples u ~ Uniform[0, cq(s)]\n",
    "############################################################\n",
    "uniform_samples = c * q.probs(proposal_samples)\n",
    "############################################################\n",
    "# TODO 2. Compute accepted and rejected samples\n",
    "############################################################\n",
    "# Bug:  Dimension out of range (expected to be in range of [-1, 0], but got 1)\n",
    "tilde_samples = p_tilde.probs(proposal_samples) \n",
    "mask = uniform_samples <= tilde_samples # Use a mask to find the accepted and rejected samples\n",
    "accepted = proposal_samples[mask]\n",
    "rejected = proposal_samples[~mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we visualize the accepted and rejected samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(I, p_tilde_x, label='$\\\\tilde{p}(x)$')\n",
    "plt.plot(I, c * q_x, label='$q(x)$')\n",
    "plt.scatter(accepted, uniform_samples[mask], c='g', label='Accepted')\n",
    "plt.scatter(rejected, uniform_samples[~mask], c='r', label='Rejected')\n",
    "plt.xlabel('$x$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Metropolis-Hastings MCMC (8 Points)\n",
    "In this exercise you will implement Metropolis-Hastings MCMC step by step.\n",
    "\n",
    "### Goal\n",
    "The goal of Markov Chain Monte Carlo is to sample from a target density $x \\sim p(x)$ only having access to the un-normalized target density $ = \\tilde{p}(x) = Zp(x)$ with $Z=\\int \\tilde{p}(x) dx$.\n",
    "\n",
    "### Target Density $\\tilde{p}(x)$\n",
    " Let's define the target distribution. In this case it is a Gaussian mixture model (GMM). Please note that it is straightforward to sample from a GMM. However, for demonstration purposes, we assume that it is not possible and we use rejection sampling to obtain samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetDist:\n",
    "    def __init__(self, means=[-1.5, 1.5, 2.5], stds=[0.4, 0.3, 0.5]):\n",
    "        \"\"\"\n",
    "        :param means: Means for the Gaussian mixture components\n",
    "        :param stds: Standard deviations for the Gaussian mixture components\n",
    "        \"\"\"\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "\n",
    "    def probs(self, samples):\n",
    "        \"\"\"\n",
    "        :param samples: Samples we want to evaluate ̃p(x)\n",
    "        :return: ̃p(x)\n",
    "        \"\"\"\n",
    "        if samples.dim() == 1:\n",
    "            samples = samples.reshape(1, -1)\n",
    "\n",
    "        gaussian_log_probs = [Normal(loc=self.means[i], scale=self.stds[i]).log_prob(samples) for i in range(len(self.means))]\n",
    "        return torch.exp(torch.logsumexp(torch.cat(gaussian_log_probs,1), 1) - math.log(len(self.means)))\n",
    "\n",
    "# Let us plot the target density\n",
    "x = torch.linspace(-5, 5, 100).reshape(-1,1)\n",
    "plt.plot(x, TargetDist().probs(x))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Proposal Distribution $q(x|x_t)$ (3 Points)\n",
    "Next, we need to define a proposal distribution $q(x|x_t)$ where we can easily sample from. In this exercise we use a Gaussian distribution $q(x|x_t) = \\mathcal{N}(x_t, \\sigma^2)$. For the proposal distribution we need to operations:\n",
    "- We need to be able to evaluate the likelihood $q(x|x_t)$\n",
    "- We need to be able to sample from $q(x|x_t)$\n",
    "\n",
    "Implement these operations in the following template class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "class GaussianProposalDist:\n",
    "    def __init__(self, proposal_std=1):\n",
    "        \"\"\"\n",
    "        :param proposal_std: Standard deviation of the proposal distribution\n",
    "        \"\"\"\n",
    "        self.proposal_std = proposal_std\n",
    "\n",
    "    def probs(self, current_location, samples):\n",
    "        \"\"\"\n",
    "        :param current_location: Current location x_t of the proposal distribution q(x|x_t)\n",
    "        :param samples: Samples x to evaluate q(x|x_t)\n",
    "        :return: q(x|x_t)\n",
    "        \"\"\"\n",
    "        ############################################################\n",
    "        # TODO Compute q(x|x_t)\n",
    "        ############################################################\n",
    "        diff = samples - current_location\n",
    "        exponent = -0.5 * (diff / self.proposal_std) ** 2\n",
    "        normalization = 1.0 / (self.proposal_std * math.sqrt(2 * math.pi))\n",
    "        return normalization * torch.exp(exponent)\n",
    "\n",
    "\n",
    "\n",
    "    def sample(self, current_location):\n",
    "        \"\"\"\n",
    "        :param current_location: Current location x_t of the proposal distribution q(x|x_t)\n",
    "        :return: x ~ q(x|x_t)\n",
    "        \"\"\"\n",
    "        ############################################################\n",
    "        # TODO Draw samples x ~ q(x|x_t)\n",
    "        ############################################################\n",
    "        return current_location + torch.randn_like(current_location) * self.proposal_std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Sampling (5 Points)\n",
    "\n",
    "Now we have everything to perform Metropolis Hastings MCMC which involves the following steps to obtain one sample:\n",
    "1. Given current location $x_t$\n",
    "2. Draw proposal $x \\sim q(x|x_t)$\n",
    "3. Evaluate $r = \\tilde{p}(x)/\\tilde{p}(x_t)$ (as $q(x|x_t) = q(x_t|x)$)\n",
    "4. If: $ r \\geq 1$: $x_{t+1} \\leftarrow x$\n",
    "5. Else: accept with probability $r$ ($x_{t+1} \\leftarrow x$), stay with probability $1-r$ ($x_{t+1} \\leftarrow x_t$)\n",
    "\n",
    "In the following you are tasked to implement these three steps. Save the accepted as well as the rejected samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 1000\n",
    "PROPOSAL_STD = 1.\n",
    "initial_location = torch.tensor([0.])\n",
    "\n",
    "proposal_dist = GaussianProposalDist(PROPOSAL_STD)\n",
    "target_dist = TargetDist()\n",
    "\n",
    "sample_dim = initial_location.shape[0]  # Dimensionality of x\n",
    "samples = torch.zeros([N_SAMPLES, sample_dim])  # Fill this tensor with samples\n",
    "current_location = initial_location  # x_t for t = 0\n",
    "for i in range(N_SAMPLES):\n",
    "    ############################################################\n",
    "    # TODO 2. Draw sample x ~ q(x|x_t)\n",
    "    ############################################################\n",
    "    proposal = proposal_dist.sample(current_location)\n",
    "    ############################################################\n",
    "    # TODO 3. Evaluate acceptance ration r\n",
    "    ############################################################\n",
    "    numerator = target_dist.probs(proposal) * proposal_dist.probs(proposal, current_location)\n",
    "    denominator = target_dist.probs(current_location) * proposal_dist.probs(current_location, proposal)\n",
    "    r = numerator / denominator\n",
    "    ############################################################\n",
    "    # TODO 4. Check if r >= 1. If so, accept the sample\n",
    "    ############################################################\n",
    "    if r >= 1.:\n",
    "        samples[i] = proposal\n",
    "        current_location = proposal\n",
    "    else:\n",
    "    ############################################################\n",
    "    # TODO 5. If r < 1 accept the sample with probability r else stay\n",
    "    ############################################################\n",
    "        accept = torch.rand(1) < r\n",
    "        if accept:\n",
    "            samples[i] = proposal\n",
    "            current_location = proposal\n",
    "        else:\n",
    "            samples[i] = current_location"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lastly, we visualize the samples obtained by the Metropolis-Hastings MCMC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-5, 5, 100).reshape(-1, 1)\n",
    "u = torch.rand(N_SAMPLES) * target_dist.probs(samples)\n",
    "plt.plot(x, target_dist.probs(x), label='$\\\\tilde{p}(x)$')\n",
    "plt.scatter(samples, u, c='g', label='Samples')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Bayesian Neural Networks (BNN) 14 Points\n",
    "\n",
    "Ge Li, ge.li@kit.edu\n",
    "\n",
    "## Guideline\n",
    "In this exercise, you are going to implement a few BNN algorithms and compare their predictions in a binary classification task. We use PyTorch as our neural network toolbox and structure the algorithms using the Object-Oriented-Programming manner.\n",
    "\n",
    "The algorithms include: MAP, SWAG, MCD and their corresponding ensembles, i.e. Deep Ensemble, Multi-SWAG, Multi-MCD.\n",
    "<br>\n",
    "\n",
    "### Big Picture and Workflow\n",
    "- We offered a few utilities functions to load dataset, evaluate models and plot classification results.\n",
    "- We offered the Maximum A Posteriori (MAP) binary classifier as our base algorithm.\n",
    "- You are going to implement a class **EnsembleWrapper** which can store multiple instances of a single algorithm to make them an ensemble model.\n",
    "- You are going to implement SWAG and MCD methods. They inherit the MAP class and can thus reuse part of the code.\n",
    "<br>\n",
    "\n",
    "### Acknowledgement\n",
    "Special thanks to Florian Seligmann. Some of the content in this homework is adapted from his bachelor's thesis codebase.\n",
    "\n",
    "### Install Dependencies\n",
    "We need to install a package **tqdm** to show the progress bar of our training and plotting process.\n",
    "Call **conda install -c conda-forge tqdm** to install it.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Plot Dataset\n",
    "We will use our best friend **Two-Moon Dataset** for our homework. This time, we added more noise to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from util import plot_dataset\n",
    "\n",
    "two_moon_dataset = np.load(\"bnn_dataset.npz\", allow_pickle=True)\n",
    "train_data_x, train_data_y = two_moon_dataset[\"train_data_x\"], two_moon_dataset[\n",
    "    \"train_data_y\"]\n",
    "val_data_x, val_data_y = two_moon_dataset[\"val_data_x\"], two_moon_dataset[\n",
    "    \"val_data_y\"]\n",
    "test_data_x, test_data_y = two_moon_dataset[\"test_data_x\"], two_moon_dataset[\n",
    "    \"test_data_y\"]\n",
    "\n",
    "# Transfer numpy array into torch tensor\n",
    "train_samples_tensor = torch.as_tensor(train_data_x, dtype=torch.float32)\n",
    "train_labels_tensor = torch.as_tensor(train_data_y, dtype=torch.long)\n",
    "test_samples_tensor = torch.as_tensor(val_data_x, dtype=torch.float32)\n",
    "test_labels_tensor = torch.as_tensor(val_data_y, dtype=torch.long)\n",
    "\n",
    "# Plotting\n",
    "plot_dataset(train_data_x, train_data_y, val_data_x, val_data_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "We define a few hyperparameters here. You can change them to see how they affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05  # Learning rate\n",
    "wd = 1e-4  # Weight decay, the L2 regularization term\n",
    "num_h_neuron = 64  # Number of neurons in the hidden layers of the NN\n",
    "dropout_p = 0.25  # The probability of a neuron being dropped out\n",
    "max_epoch = 1000  # Max training episode\n",
    "seed = 0  # Random seed to ensure reproducibility\n",
    "swag_start = max_epoch - 600  # From this epoch will SWAG start updating\n",
    "swag_update_interval = 5  # SWAG update interval\n",
    "n_ensemble = 5  # Number of method instances in the Multi-X models.\n",
    "num_smp = 100  # Number of parameter samples from the posterior distribution of each single method instance\n",
    "plot_density = 0.02  # The pixel size of the contour. Use bigger values if you face problems in plotting the result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum A Posteriori\n",
    "We define **MAPBinaryClassifier** to perform binary classification following the MAP manner. As introduced in the lecture, MAP is equivalent to train a neural network with L2 regularization.\n",
    "This class includes 4 functions:\n",
    "- init: instantiate one net and one optimizer for the model\n",
    "- train_one_epoch: apply one train iteration using the training and validation datasets. Here, as our datasets are rather small, we feed the entire datasets without using minibatch.\n",
    "- step: given input and label, compute the loss function and accuracy of the classification\n",
    "- inference: given input, compute the probabilities of classification\n",
    "\n",
    "This class serves as the super class and will be further extended in **SWAGBinaryClassifier** and **MCDBinaryClassifier** classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "class MAPBinaryClassifier:\n",
    "    def __init__(self, **kwargs):\n",
    "        # nn.Sequential is a container for a sequence of neural network layers\n",
    "        # Call self.net(x) will sequentially apply the layers to the input x\n",
    "        self.net = nn.Sequential(nn.Linear(2, num_h_neuron),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_h_neuron, num_h_neuron),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_h_neuron, num_h_neuron),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(num_h_neuron, 2),\n",
    "                                 nn.LogSoftmax(dim=-1))\n",
    "\n",
    "        # SGD Optimizer, lr is the learning rate,\n",
    "        # weight_decay is the L2 regularization factor\n",
    "        self.optimizer = SGD(self.net.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    def train_one_epoch(self, train_x, train_y, val_x, val_y):\n",
    "        \"\"\"\n",
    "        Apply one train iteration using the training and validation datasets.\n",
    "        Here we feed the entire datasets without using minibatch.\n",
    "        \"\"\"\n",
    "\n",
    "        # Training\n",
    "        self.net.train() # Set the model to training mode\n",
    "        self.optimizer.zero_grad()\n",
    "        loss, train_accuracy = self.step(train_x, train_y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        self.net.eval() # Set the model to evaluation mode\n",
    "        with torch.no_grad(): # Turn off gradient computation under this context\n",
    "            _, val_accuracy = self.step(val_x, val_y)\n",
    "\n",
    "        # Return\n",
    "        return train_accuracy, val_accuracy\n",
    "\n",
    "    def step(self, x, y):\n",
    "        \"\"\"\n",
    "        Given input and label, compute the cross-entropy loss function and accuracy\n",
    "        \"\"\"\n",
    "        pred_log_prob = self.net(x)\n",
    "        pred_class = pred_log_prob.argmax(dim=-1)\n",
    "        loss = nn.functional.nll_loss(pred_log_prob, y)\n",
    "        num_data = x.shape[0]\n",
    "        num_pred_correct = (pred_class == y).sum().item()\n",
    "        accuracy = num_pred_correct / num_data\n",
    "        return loss, accuracy\n",
    "\n",
    "    # The method decorator @torch.no_grad() disables gradient computation\n",
    "    @torch.no_grad()\n",
    "    def inference(self, x):\n",
    "        self.net.eval()  # Set the model to evaluation mode\n",
    "        # Compute the probability of each class given the log-probability\n",
    "        pred_prob = self.net(x).exp()\n",
    "        return pred_prob\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Ensemble\n",
    "A Deep Ensemble model contains multiple MAP model instances. When doing inference, the predictions of each instance will be averaged as the resulting prediction.\n",
    "\n",
    "### Task 3.1: Ensemble wrapper (2 Points)\n",
    "Indeed, any Multi-X methods can be implemented in a similar way, i.e. using an ensemble wrapper to make a single method \"Multi\". Therefore, in this homework, we will firstly implement this Ensemble Wrapper, and then always use this wrapper to instantiate our method. If the number of ensemble instances is 1, the resulting method is just a single method. Otherwise, it is a Multi-X method.\n",
    "\n",
    "For convenience and simplicity, we also implement the main training loop in this wrapper class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm is a package offering nice look progress bar\n",
    "from tqdm import tqdm\n",
    "# We always set the random seed to ensure reproducibility\n",
    "from util import set_random_seed_globally\n",
    "\n",
    "\n",
    "class EnsembleWrapper:\n",
    "    def __init__(self, num_ensemble, model_class, **kwargs):\n",
    "        \"\"\"\n",
    "        Create method ensemble\n",
    "        Args:\n",
    "            num_ensemble: the number of base models, 1 for single method, > 1 for Multi-method\n",
    "            model_class: class name of the single method\n",
    "            **kwargs: other keyword arguments specific to the single method init function\n",
    "        \"\"\"\n",
    "        self.n_ensemble = num_ensemble\n",
    "        set_random_seed_globally(seed)\n",
    "        self.models = [model_class(**kwargs) for _ in range(num_ensemble)]\n",
    "\n",
    "    def train_model(self, train_x, train_y, val_x, val_y):\n",
    "        \"\"\"\n",
    "        Train all model instances in the ensemble\n",
    "        \"\"\"\n",
    "\n",
    "        # Loop over models\n",
    "        for i, model in enumerate(self.models):\n",
    "\n",
    "            # Loop over training iterations\n",
    "            pbar = tqdm(range(max_epoch))  # progress bar\n",
    "\n",
    "            for epoch in pbar:\n",
    "                train_accuracy, val_accuracy = \\\n",
    "                    model.train_one_epoch(train_x, train_y, val_x, val_y)\n",
    "\n",
    "                # Logging and updating progress bar\n",
    "                pbar.set_postfix({\"Model\": i + 1, \"Epoch\": epoch + 1,\n",
    "                                  \"train_accuracy\": train_accuracy,\n",
    "                                  \"val_accuracy\": val_accuracy})\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ensemble_inference(self, x):\n",
    "        \"\"\"\n",
    "        Aggregate inference result of each model instance\n",
    "        \"\"\"\n",
    "        ######################### Your code starts here #########################\n",
    "        # Loop over each method instance\n",
    "        # Todo\n",
    "        ensemble_predictions = []\n",
    "        for model in self.models:\n",
    "            predictions = model.inference(x)\n",
    "            ensemble_predictions.append(predictions)\n",
    "        # Aggregate the predictions\n",
    "        # Todo\n",
    "        ensemble_predictions = torch.stack(ensemble_predictions, dim=0)\n",
    "        ensemble_prediction = ensemble_predictions.mean(dim=0)\n",
    "        ######################### Your code ends here #########################\n",
    "        return ensemble_prediction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP and DeepEnsemble-5 instances\n",
    "Verify your implementation by training a MAP instance and a DeepEnsemble-5 instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# MAP, num_ensemble = 1\n",
    "max_a_p = EnsembleWrapper(num_ensemble=1,\n",
    "                          model_class=MAPBinaryClassifier)\n",
    "print(\"Training MAP\")\n",
    "time.sleep(0.3) # Sleep for 0.3 second to ensure the print function won't break the progress bar\n",
    "max_a_p.train_model(train_samples_tensor, train_labels_tensor,\n",
    "                    test_samples_tensor, test_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Ensemble, we ensemble 5 MAP instances\n",
    "deep_ensemble_5 = EnsembleWrapper(num_ensemble=n_ensemble,\n",
    "                                  model_class=MAPBinaryClassifier)\n",
    "print(\"Training DeepEnsemble5\")\n",
    "time.sleep(0.3)\n",
    "deep_ensemble_5.train_model(train_samples_tensor, train_labels_tensor,\n",
    "                            test_samples_tensor, test_labels_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Weight Averaging Gaussian (SWAG)\n",
    "SWAG uses the parameters of the NN at different training stages to fit the approximated posterior distribution, known as a Multivariate Gaussian distribution. We implement it by using and extending the MAP as the super class.\n",
    "<br>\n",
    "\n",
    "The training procedure of a SWAG can be described as:\n",
    "- Train a SWAG as we train an MAP classifier until convergence.\n",
    "- Then keep training it and collect the parameters of the NN at different further $T$ training stages.\n",
    "- The parameters collected at a certain stage (reshaped as a parameter vector $\\boldsymbol\\theta_i$ ) can be considered as a sample of the approximated posterior distribution.\n",
    "- We use all these collected samples to compute the posterior distribution, in terms of the mean and covariance.\n",
    "- With mean $\\overline{\\boldsymbol\\theta} = \\frac{1}{T} \\sum^T_{i=1}\\boldsymbol\\theta_i$ and covariance $\\boldsymbol\\Sigma = \\frac{1}{T-1} \\sum^T_{i=1}(\\boldsymbol\\theta_i - \\boldsymbol\\theta)(\\boldsymbol\\theta_i - \\boldsymbol\\theta)^\\intercal = \\frac{1}{T-1} \\boldsymbol{D} \\boldsymbol{D}^\\intercal$\n",
    "- To decrease memory usage, we avoid saving all individual samples. Instead, each time when we get a new sample, we update a few intermediate variables, including the running mean $\\overline{\\boldsymbol\\theta}$, mean of square $\\overline{\\boldsymbol\\theta^2}$ and the deviation $\\boldsymbol d$ of the parameter vector. We store the last K deviations, resulting a low-rank deviation matrix $\\boldsymbol{D}_{lr}$.\n",
    "\n",
    "\\begin{align*}\n",
    "        \\text{Update mean:} \\quad \\overline{\\boldsymbol\\theta} \\leftarrow \\frac{i \\overline {\\boldsymbol\\theta} + {\\boldsymbol\\theta_i}}{i + 1} \\\\\n",
    "        \\text{Update mean square:} \\quad \\overline{\\boldsymbol\\theta^2} \\leftarrow \\frac{i \\overline {\\boldsymbol\\theta^2} + {\\boldsymbol\\theta^2_i}}{i + 1} \\\\\n",
    "        \\text{Compute deviation:} \\quad {\\boldsymbol d_i} = \\boldsymbol\\theta - \\overline{\\boldsymbol\\theta_i}\\\\\n",
    "        \\text{Store deviation:} \\quad {\\boldsymbol D_{lr}}.\\text{pop(0)}, \\quad {\\boldsymbol D_{lr}}.\\text{add}{(\\boldsymbol d_i)}\n",
    "\\end{align*}\n",
    "<br>\n",
    "\n",
    "The inference procedure can be described as:\n",
    "- Approximate the covariance matrix $\\boldsymbol\\Sigma$ using the low-rank deviation matrix $\\boldsymbol{D}_{lr}$ and a diagonal covariance matrix $\\boldsymbol\\Sigma_{\\text{diag}} = \\text{diag}(\\overline{\\boldsymbol\\theta^2} - {\\overline{\\boldsymbol\\theta}}^2)$.\n",
    "- Form up the posterior distribution using $\\overline{\\boldsymbol\\theta}$ and approximated $\\boldsymbol\\Sigma$.\n",
    "- Generate parameter samples using the posterior, iteratively reset the network and make a prediction using these parameter samples.\n",
    "- Averaging all these individual predictions and use the result as the final prediction.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: SWAG implementation (4 + 5 = 9 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two utility functions to manipulate the parameters of the NN\n",
    "from util import parameters_to_vector, set_params_to_net\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object-Oriented-Programming (OOP)\n",
    "# We inherit MAPBinaryClassifier as our super class of SWAG\n",
    "class SWAGBinaryClassifier(MAPBinaryClassifier):\n",
    "    def __init__(self, num_samples, swag_start_epoch, update_interval, k=30):\n",
    "        # We reuse the init function of the super class to create our NN and optimizer (OOP)\n",
    "        super().__init__()\n",
    "\n",
    "        # The last K columns of the Deviation Matrix\n",
    "        self.K = k\n",
    "\n",
    "        # The intermediate running variables, later will be used to compute the posterior distribution\n",
    "        # The mean of the parameter samples\n",
    "        # Shape: [num_params]\n",
    "        self.swag_mean = parameters_to_vector(self.params)\n",
    "\n",
    "        # The mean of the squared parameter samples\n",
    "        # Shape: [num_params]\n",
    "        self.swag_square = self.swag_mean.pow(2)\n",
    "\n",
    "        # The Low rank deviation matrix, storing the K latest deviations (parameter sample to the mean)\n",
    "        # Shape: [num_params, K]\n",
    "        self.swag_dev = torch.zeros((self.swag_mean.shape[0], self.K))\n",
    "\n",
    "        # Helper variables to determine when and how frequent to update the SWAG distribution variables\n",
    "        self.swag_start_epoch = swag_start_epoch\n",
    "        self.update_interval = update_interval\n",
    "        self.num_swag_update = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        # In the inference, how many parameter samples are going to be sampled from the computed posterior distribution\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        \"\"\"\n",
    "        Return the parameters managed by the optimizer, which are the same parameters stored in the NN.\n",
    "        \"\"\"\n",
    "        return self.optimizer.param_groups\n",
    "\n",
    "    def train_one_epoch(self, train_x, train_y, val_x, val_y):\n",
    "        \"\"\"\n",
    "        Train one epoch similar to MAP.\n",
    "        If the update condition of SWAG has been satisfied, then update the SWAG variables.\n",
    "\n",
    "        This function will override the namesake function defined in the super class (OOP).\n",
    "        \"\"\"\n",
    "\n",
    "        # Update network as MAP, reuse the super class function\n",
    "        train_accuracy, val_accuracy = super().train_one_epoch(train_x, train_y,\n",
    "                                                               val_x, val_y)\n",
    "\n",
    "        # Update SWAG if certain conditions have been satisfied\n",
    "        if self.current_epoch >= self.swag_start_epoch \\\n",
    "                and self.current_epoch % self.update_interval == 0:\n",
    "            self.update_swag_params()\n",
    "        self.current_epoch += 1\n",
    "\n",
    "        # Return\n",
    "        return train_accuracy, val_accuracy\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_swag_params(self):\n",
    "        \"\"\"\n",
    "        Update SWAG intermediate variables using the latest parameters stored in the NN\n",
    "        \"\"\"\n",
    "        # We flatten all parameters from different NN layers into a parameter vector\n",
    "        params_vec = parameters_to_vector(self.params)\n",
    "\n",
    "        ######################### Your code starts here #########################\n",
    "        # Update the mean of the parameter samples\n",
    "        # Todo\n",
    "        self.swag_mean = (self.num_swag_update * self.swag_mean + params_vec) / (self.num_swag_update + 1)\n",
    "        # Update the mean of the squared parameter samples\n",
    "        # Todo\n",
    "        self.swag_square = (self.num_swag_update * self.swag_square + params_vec.pow(2)) / (self.num_swag_update + 1)\n",
    "\n",
    "        # Update the Deviation matrix by only keeping the latest K columns and discard the rest\n",
    "        # You may need to use torch.roll() to shift the columns and add the new deviation vector to the last column\n",
    "        # Todo\n",
    "        self.swag_dev = torch.roll(self.swag_dev, shifts=-1, dims=1)\n",
    "        self.swag_dev[:, -1] = params_vec - self.swag_mean\n",
    "        ######################### Your code ends here #########################\n",
    "\n",
    "        # Increase the number of SWAG updates\n",
    "        self.num_swag_update += 1\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_approx_posterior(self):\n",
    "        \"\"\"\n",
    "        Fit the posterior distribution, modeled as a low-rank multivariate Gaussian distribution\n",
    "        \"\"\"\n",
    "        eps = 1e-6 # For numerical stability\n",
    "        cov_diag = 0.5 * torch.relu((self.swag_square - self.swag_mean.pow(2)) + eps)\n",
    "        cov_factor = self.swag_dev / math.sqrt(2 * (self.K - 1))\n",
    "        posterior = torch.distributions.LowRankMultivariateNormal(\n",
    "            self.swag_mean, cov_factor, cov_diag, validate_args=False)\n",
    "        return posterior\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, x):\n",
    "        \"\"\"\n",
    "        Make prediction given input.\n",
    "        This function will override the namesake function in the super class (OOP).\n",
    "        \"\"\"\n",
    "\n",
    "        ######################### Your code starts here #########################\n",
    "        # Get the posterior distribution\n",
    "        # Todo\n",
    "        posterior = self.get_approx_posterior()\n",
    "        # Sample parameter samples\n",
    "        # Todo\n",
    "        params_smp = posterior.rsample((self.num_samples,))\n",
    "        # Make a tensor to store all predictions\n",
    "        # Todo\n",
    "        predictions = torch.zeros((self.num_samples, x.size(0),x.size(1)))\n",
    "        # Compute and store each individual prediction\n",
    "        for i, smp in enumerate(params_smp):\n",
    "            # Set parameter sample to NN\n",
    "            # Todo\n",
    "            # Missing: \n",
    "            set_params_to_net(smp,self.params)\n",
    "            # Make and store prediction using the network\n",
    "            # Todo\n",
    "            # Missing: \n",
    "            output = self.net(x)\n",
    "            #print(len(params_smp),output.shape)\n",
    "            #print(predictions.size())\n",
    "            predictions[i] = output\n",
    "        ######################### Your code ends here #########################\n",
    "\n",
    "        return predictions.mean(dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWAG and Multi-SWAG instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single SWAG\n",
    "swag = EnsembleWrapper(num_ensemble=1,\n",
    "                       model_class=SWAGBinaryClassifier,\n",
    "                       num_samples=num_smp,\n",
    "                       swag_start_epoch=swag_start,\n",
    "                       update_interval=swag_update_interval)\n",
    "print(\"Training SWAG\")\n",
    "time.sleep(0.3)\n",
    "swag.train_model(train_samples_tensor, train_labels_tensor,\n",
    "                 test_samples_tensor, test_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi SWAG, num_ensemble = 5 (by default)\n",
    "multi_swag_5 = EnsembleWrapper(num_ensemble=n_ensemble,\n",
    "                               model_class=SWAGBinaryClassifier,\n",
    "                               num_samples=num_smp,\n",
    "                               swag_start_epoch=swag_start,\n",
    "                               update_interval=swag_update_interval)\n",
    "print(\"Training Multi-SWAG-5\")\n",
    "time.sleep(0.3)\n",
    "multi_swag_5.train_model(train_samples_tensor, train_labels_tensor,\n",
    "                         test_samples_tensor, test_labels_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Dropout (MCD)\n",
    "Dropout is a regularization technique used in NN to mitigate overfitting. It randomly deactivates a fraction of the units during training, forcing the network to learn more robust representations. This prevents co-adaptation of neurons and improves generalization. In inference, the dropout is often disabled. Monte Carlo dropout extends this approach by performing dropout even in inference, and average the result of multiple forward passes to provide a measure of uncertainty.\n",
    "\n",
    "In PyTorch, eval() and train() are methods used to control the behavior of a neural network model.\n",
    "eval() sets the NN in evaluation mode and disables certain operations like dropout and batch normalization, ensuring deterministic behavior and consistent results during inference. train() sets the NN in training mode and enables dropout and batch normalization to improve generalization. Based on the above information, how can we enable dropout in inference mode?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Monte-Carlo Dropout (MCD) implementation (3 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object-Oriented-Programming (OOP)\n",
    "# We inherit MAPBinaryClassifier as our super class of MCD\n",
    "class MCDBinaryClassifier(MAPBinaryClassifier):\n",
    "    def __init__(self, num_samples):\n",
    "        # We override the init function of the super class to create our NN with Dropout layers\n",
    "        self.net = nn.Sequential(nn.Linear(2, num_h_neuron),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout_p),\n",
    "                                 nn.Linear(num_h_neuron, num_h_neuron),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout_p),\n",
    "                                 nn.Linear(num_h_neuron, num_h_neuron),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout_p),\n",
    "                                 nn.Linear(num_h_neuron, 2),\n",
    "                                 nn.LogSoftmax(dim=-1))\n",
    "\n",
    "        self.optimizer = SGD(self.net.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        # Number of samples used in inference\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, x):\n",
    "        \"\"\"\n",
    "        Make prediction given input.\n",
    "        This function will override the namesake function in the super class (OOP).\n",
    "        \"\"\"\n",
    "        ######################### Your code starts here #########################\n",
    "        # Set model train mode to enable dropout in inference\n",
    "        # Todo\n",
    "        self.net.train()\n",
    "        # Make a tensor to store all predictions\n",
    "        # Todo\n",
    "        predictions = torch.zeros((self.num_samples, x.size(0), x.size(1)))\n",
    "\n",
    "        # Compute and store each individual prediction\n",
    "        # Todo\n",
    "        for i in range(self.num_samples):\n",
    "            output = self.net(x)\n",
    "            predictions[i] = output\n",
    "        ######################### Your code ends here #########################\n",
    "        return predictions.mean(dim=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCD and Multi-MCD instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcd = EnsembleWrapper(num_ensemble=1,\n",
    "                      model_class=MCDBinaryClassifier,\n",
    "                      num_samples=num_smp)\n",
    "print(\"Training MCD\")\n",
    "time.sleep(0.3)\n",
    "mcd.train_model(train_samples_tensor, train_labels_tensor,\n",
    "                test_samples_tensor, test_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_mcd_5 = EnsembleWrapper(num_ensemble=n_ensemble,\n",
    "                              model_class=MCDBinaryClassifier,\n",
    "                              num_samples=num_smp)\n",
    "print(\"Training Multi-MCD-5\")\n",
    "time.sleep(0.3)\n",
    "multi_mcd_5.train_model(train_samples_tensor, train_labels_tensor,\n",
    "                        test_samples_tensor, test_labels_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and compare all models\n",
    "**This may take a few minutes to finish**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import plot_all_models, evaluate_models\n",
    "model_dict = {\n",
    "    \"MAP\": max_a_p,\n",
    "    \"DeepEnsemble5\": deep_ensemble_5,\n",
    "    \"SWAG\": swag,\n",
    "    \"Multi-SWAG-5\": multi_swag_5,\n",
    "    \"MCD\": mcd,\n",
    "    \"Multi-MCD-5\": multi_mcd_5\n",
    "}\n",
    "evaluate_models(model_dict, test_data_x, test_data_y)\n",
    "plot_all_models(model_dict, train_data_x, train_data_y, plot_density)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have done everything correctly, you should gain roughly 93% of the test accuracy for all models. We also implemented the adapted callibration error (ACE) in the util file. However, as our task is rather simple, a comparison of callibration errors is not very meaningful.\n",
    "\n",
    "The resulting classification boundaries of all models should be very different. The MAP has a very narrow boundary while the others get more uncertainty in the area that is far from the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
