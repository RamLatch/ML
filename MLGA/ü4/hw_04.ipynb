{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: ML - Grundlagen und Algorithmen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.) Reminders\n",
    "Please adhere to the hand-in conventions specified in the 0-th exercise sheet, i.p.,\n",
    "- You have to **submit the jupyter notebook file as well as the PDF**! \n",
    "- Please **adhere to the zip naming conventions**!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Learning\n",
    "We start by loading our regression data set for the exercises on Bayesian learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "x_plot = np.load('x_plot.npy')\n",
    "y_plot = np.load('y_plot.npy')\n",
    "\n",
    "# we assume the data noise standard deviation is known with a value of 1.0 \n",
    "# thus, for this exercise, we fix the likelihood standard deviation sigma_y to 1.0\n",
    "sigma_y = 1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Bayesian Linear Regression (15 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will implement Bayesian linear regression (BLR). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1) Radial Basis Function Features (3 Points)\n",
    "We will use radial basis function (RBF) features in this exercise. Remember the definition of the *RBF feature function*,\n",
    "$$\n",
    "\\phi_i(\\boldsymbol{x}):=\\exp\\left(-\\dfrac{||\\boldsymbol{x}-\\boldsymbol{\\mu}_i||^2}{2\\sigma^2}\\right), \\quad i\\in \\left\\{1, \\dots, k \\right\\}, \\quad \\phi_0(\\boldsymbol{x}) := 1, \\quad \\boldsymbol{x}, \\boldsymbol{\\mu_i} \\in \\mathbb R^d, \\sigma \\in \\mathbb R.\n",
    "$$\n",
    "Here, all features share the same variance (\"bandwith\") $\\sigma^2$ and we define the $i$-th feature to have mean $\\boldsymbol{\\mu}_i$. Do not confuse $\\sigma^2$, the bandwith of the RBF kernel, with the likelihood variance $\\sigma_y^2$, which we fixed above. The $0$-th feature is the bias. \n",
    "\n",
    "We define the *feature vector* as \n",
    "$$\n",
    "\\boldsymbol{\\phi}(\\boldsymbol{x}) := \\left(\\phi_0(\\boldsymbol{x}), \\phi_1 (\\boldsymbol{x}), \\dots, \\phi_k (\\boldsymbol{x}) \\right)^T \\in \\mathbb R^{k+1}.\n",
    "$$\n",
    "\n",
    "Given training inputs $\\boldsymbol X = \\left\\{\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_N\\right\\}$, $\\boldsymbol{x}_i \\in \\mathbb R^d$, we define the *feature matrix* as:\n",
    "$$\n",
    "\\boldsymbol{\\Phi}(\\boldsymbol X) := \n",
    "\\begin{pmatrix}\n",
    " \\boldsymbol{\\phi}(\\boldsymbol{x}_1)^T \\\\\n",
    "    \\vdots \\\\\n",
    " \\boldsymbol{\\phi}(\\boldsymbol{x}_N)^T \\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    " \\phi_0(\\boldsymbol{x}_1) &\\phi_1(\\boldsymbol{x}_1) & \\phi_2(\\boldsymbol{x}_1) &...& \\phi_k(\\boldsymbol{x}_1) \\\\\n",
    "    \\vdots &\\vdots &\\vdots &\\vdots &\\vdots\\\\\n",
    "  \\phi_0(\\boldsymbol{x}_N) &\\phi_1(\\boldsymbol{x}_N) & \\phi_2(\\boldsymbol{x}_N) &...& \\phi_k(\\boldsymbol{x}_N) \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Implement the following function, which is supposed to return the RBF feature matrix.\n",
    "\n",
    "**Hint**: As always, do not use any for-loops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_feature_matrix(x: np.ndarray, means: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param x: inputs (shape: [N, d])\n",
    "    :param means: means of the RBF features (shape: [k, d])\n",
    "    :param sigma: bandwidth parameter of the RBF features (shape: scalar)\n",
    "    :return: the RBF feature matrix (shape: [N, k+1])\n",
    "    \"\"\"\n",
    "    if len(x.shape) == 1:\n",
    "        x = x.reshape((-1, 1))\n",
    "\n",
    "    if len(means.shape) == 1:\n",
    "        means = means.reshape((-1, 1))\n",
    "\n",
    "    ############################################################\n",
    "    # TODO Implement the RBF features\n",
    "    dist = np.linalg.norm(x[:, np.newaxis] - means, axis=-1)\n",
    "    features = np.exp(-0.5 * (dist / sigma) ** 2)\n",
    "    features = np.hstack((features, np.ones((features.shape[0], 1))))\n",
    "    ############################################################\n",
    "    \n",
    "    assert features.shape == (x.shape[0], means.shape[0]+1)\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we fix the number $k$ of RBF features as well as their means $\\boldsymbol{\\mu}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a fixed number of k = 5 RBF features\n",
    "k = 5 \n",
    "# we spread the centers of the RBF over the range of x_plot\n",
    "feature_means = np.linspace(np.min(x_plot), np.max(x_plot), k)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True, figsize=(15,5))\n",
    "feature_sigmas = [0.1, 0.5, 1.0]\n",
    "features = [rbf_feature_matrix(x=x_plot, means=feature_means, sigma=feature_sigma) for feature_sigma in feature_sigmas]\n",
    "for ax, feature, feature_sigma in zip(axes, features, feature_sigmas):\n",
    "    ax.plot(x_plot, feature[:, :])\n",
    "    ax.set_title(f\"Features ($\\sigma$ = {feature_sigma})\")\n",
    "    _ = ax.set_xlabel(\"$x$\")\n",
    "    _ = ax.set_ylabel(\"$\\phi_i(x)$\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2) Posterior Distribution (3 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement the posterior distribution of the parameters for BLR. Recall that in the case of a Gaussian prior and a Gaussian likelihood, the posterior distribution is again Gaussian. In the lecture, we derived the closed form solution for the posterior mean and covariance parameters for a zero-mean prior with isotropic precision $\\lambda$:\n",
    "$$\n",
    "\\boldsymbol \\mu_{\\boldsymbol w | \\boldsymbol X, \\boldsymbol{y}} = \\left( \\boldsymbol{\\Phi}(\\boldsymbol{X})^T \\boldsymbol{\\Phi}(\\boldsymbol{X}) + \\sigma_y^2 \\lambda \\boldsymbol I \\right)^{\\,\\,-1} \\boldsymbol{\\Phi}(\\boldsymbol{X})^T \\boldsymbol{y}, \\quad \\boldsymbol \\Sigma_{\\boldsymbol w | \\boldsymbol X, \\boldsymbol y} = \\sigma_y^2 \\left(\\boldsymbol{\\Phi}(\\boldsymbol{X})^T \\boldsymbol{\\Phi}(\\boldsymbol{X}) + \\sigma_y^2 \\lambda \\boldsymbol{I} \\right)^{\\,\\,-1}\n",
    "$$\n",
    "where $\\boldsymbol{y} := \\left(y_1, \\dots, y_N \\right)^T$ are the targets corresponding to $\\boldsymbol X$ and $\\sigma_y^2$ is the variance of the likelihood.\n",
    "\n",
    "Implement the following function, which should return the posterior mean and the posterior covariance using RBF features.\n",
    "\n",
    "**Hints:** \n",
    "- We defined the likelihood variance $\\sigma_y^2$ as a global variable at the beginning of this notebook and fixed it to the true noise variance of 1.0, which we assume to be known for this exercise. \n",
    "- Avoid unneccessary computations, and use numerically stable operations instead of computing inverses, whenever possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blr_posterior_distribution(X: np.ndarray, y: np.ndarray, lam: float, means: np.ndarray, sigma: float):\n",
    "    \"\"\"\n",
    "    :param X: training inputs (shape: [N, d])\n",
    "    :param y: training targets (shape: [N, 1])    \n",
    "    :param lam: prior precision (scalar)\n",
    "    :param means: means of the RBF features (shape: [k, d])\n",
    "    :param sigma: bandwidth of the RBF features (scalar)\n",
    "    :return: posterior mean (shape: [k+1])\n",
    "             posterior covariance (shape: [k+1, k+1]) \n",
    "    \"\"\"\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape((-1, 1))\n",
    "        \n",
    "    ############################################################\n",
    "    # TODO Implement the posterior distribution\n",
    "    Phi = rbf_feature_matrix(X, means, sigma)\n",
    "    inv_term = np.linalg.inv(np.dot(Phi.T, Phi) + lam * np.eye(Phi.shape[1]))\n",
    "    post_mean = np.dot(inv_term, np.dot(Phi.T, y)).squeeze()\n",
    "    post_cov = sigma**2 * inv_term\n",
    "    ############################################################\n",
    "    assert post_mean.shape == (means.shape[0]+1,)\n",
    "    assert post_cov.shape == (means.shape[0]+1, means.shape[0]+1)\n",
    "    return post_mean, post_cov"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3) Predictive Distribution (3 Points)\n",
    "Now, we will implement the BLR predictive distribution. From the lecture, we know the predictive mean and variance at a query input $\\boldsymbol{x}^* \\in \\mathbb R^d$ are given analytically as:\n",
    "$$\n",
    "\\mu\\left(\\boldsymbol{x}^* \\right) = \\boldsymbol{\\phi}\\left(\\boldsymbol{x}^*\\right)^T \\left( \\boldsymbol{\\Phi}(\\boldsymbol{X})^T \\boldsymbol{\\Phi}(\\boldsymbol{X}) + \\sigma_y^2 \\lambda \\boldsymbol I \\right)^{\\,\\,-1} \\boldsymbol{\\Phi}(\\boldsymbol{X})^T \\boldsymbol{y}, \\quad \\sigma^2\\left(\\boldsymbol{x}^* \\right) = \\sigma^2_y + \\sigma^2_y \\boldsymbol{\\phi}\\left(\\boldsymbol{x}^*\\right)^T \\left( \\boldsymbol{\\Phi}(\\boldsymbol{X})^T \\boldsymbol{\\Phi}(\\boldsymbol{X}) + \\sigma_y^2 \\lambda \\boldsymbol I \\right)^{\\,\\,-1} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}^*\\right).\n",
    "$$\n",
    "\n",
    "Implement the following function, which should return the predictive mean and the predictive variance using RBF features.\n",
    "\n",
    "**Hints:** \n",
    "- We defined the likelihood variance $\\sigma_y^2$ as a global variable at the beginning of this notebook and fixed it to the true noise variance of 1.0, which we assume to be known for this exercise. \n",
    "- Avoid unneccessary computations, and use numerically stable operations instead of computing inverses, whenever possible!\n",
    "- You can re-use `rbf_feature_matrix` to compute the feature vector at the query inputs.\n",
    "- Do not use any for loops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blr_predictive_distribution(x: np.ndarray, X: np.ndarray, y: np.ndarray, lam: float, means: np.ndarray, sigma: float):\n",
    "    \"\"\"\"\n",
    "    :param x: query inputs (shape: [N_q, d])\n",
    "    :param X: training inputs (shape: [N, d])\n",
    "    :param y: training targets (shape: [N, 1])    \n",
    "    :param lam: prior precision (scalar)\n",
    "    :param means: means of the RBF features (shape: [k, d])\n",
    "    :param sigma: bandwidth of the RBF features (scalar)\n",
    "    :return: the predictive mean (shape: [N_q])\n",
    "             the predictive variance (shape: [N_q])\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO Implement the predictive distribution\n",
    "    Phi_X = rbf_feature_matrix(X, means, sigma)\n",
    "    Phi_x = rbf_feature_matrix(x, means, sigma)\n",
    "    inv_term = np.linalg.inv(np.dot(Phi_X.T, Phi_X) + lam * np.eye(Phi_X.shape[1]))\n",
    "    mean_x = np.dot(Phi_x, np.dot(inv_term, np.dot(Phi_X.T, y))).squeeze()\n",
    "    var_x = sigma**2 + np.diagonal(np.dot(Phi_x, np.dot(inv_term, Phi_x.T)))\n",
    "    ############################################################\n",
    "    assert mean_x.shape == (x.shape[0],)\n",
    "    assert var_x.shape == (x.shape[0],)    \n",
    "    return mean_x, var_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4) Sample Functions (3 Points)\n",
    "We now implement a function to compute function samples, i.e., functions generated by models sampled from the BLR posterior distribution. We want to return $S$ function samples in one function call.\n",
    "\n",
    "**Hints**:\n",
    "- Given the training data, first compute the posterior distribution over parameters.\n",
    "- Sample from this posterior distribution a set of $S$ weight vectors. Use the appropriate function from `np.random`. \n",
    "- Transform the sampled weight vectors to function evaluations at the query inputs. You already know how to do that from our discussion of standard linear regression.\n",
    "- As always, for-loops are not allowed! `np.random` allows batched sampling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blr_sample_function(x: np.ndarray, X: np.ndarray, y: np.ndarray, lam: float, means: np.ndarray, sigma: float, S: int):\n",
    "    \"\"\"\n",
    "    :param x: query inputs (shape: [N_q, d])\n",
    "    :param X: training inputs (shape: [N, d])\n",
    "    :param y: training targets (shape: [N, 1])    \n",
    "    :param lam: prior precision (scalar)\n",
    "    :param means: means of the RBF features (shape: [k, d])\n",
    "    :param sigma: bandwidth of the RBF features (scalar)\n",
    "    :param S: number of sample functions to generate (integer) \n",
    "    :return: the predictions at the inputs for S sampled models (shape: [S, N_q])\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO Implement the computation of S function samples, evaluated at x\n",
    "    Phi_x = rbf_feature_matrix(x, means, sigma)\n",
    "    post_mean, post_cov = blr_posterior_distribution(X, y, lam, means, sigma)\n",
    "    weight_samples = np.random.multivariate_normal(post_mean, post_cov, size=S)\n",
    "    predictions = np.dot(Phi_x, weight_samples.T).T\n",
    "    ############################################################\n",
    "    assert predictions.shape == (S, x.shape[0])\n",
    "    return predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now plot some sampled functions and the predictive distribution for different values of the hyperparameters $\\lambda$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lambdas and sigmas\n",
    "lambdas = [1e-5, 1e-3, 1.0, 10.0]\n",
    "feature_sigmas = [0.1, 0.5, 1.0, 10.0]\n",
    "\n",
    "# compute predictive distribution and function samples for the lambdas and sigmas\n",
    "pred_means, pred_vars, funcss, labels = [], [], [], []\n",
    "for lam, feature_sigma in product(lambdas, feature_sigmas):\n",
    "    # obtain the predictive distribution\n",
    "    pred_mean, pred_var = blr_predictive_distribution(x=x_plot, X=x_train, y=y_train, lam=lam, \n",
    "                                                      means=feature_means, sigma=feature_sigma)\n",
    "\n",
    "    # obtain 10 sample functions\n",
    "    funcs = blr_sample_function(x=x_plot, X=x_train, y=y_train, lam=lam, \n",
    "                                means=feature_means, sigma=feature_sigma, S=10)\n",
    "        \n",
    "    # collect computed stuff\n",
    "    pred_means.append(pred_mean)\n",
    "    pred_vars.append(pred_var)\n",
    "    funcss.append(funcs)\n",
    "    labels.append(f\"$\\lambda$ = {lam}, $\\sigma$ = {feature_sigma}\")\n",
    "    \n",
    "\n",
    "# plot \n",
    "fig, axes = plt.subplots(nrows=len(lambdas), ncols=len(feature_sigmas), sharex=True, figsize=(len(feature_sigmas)*5, len(lambdas)*3), squeeze=False)\n",
    "for i, (pred_mean, pred_var, funcs, label) in enumerate(zip(pred_means, pred_vars, funcss, labels)):\n",
    "    ax = axes[i//len(feature_sigmas), i%len(feature_sigmas)]\n",
    "    \n",
    "    # the predictive distribution together with the 95% confidence interval\n",
    "    ax.plot(x_plot, pred_mean, 'b', label=label)\n",
    "    ax.fill_between(np.squeeze(x_plot), np.squeeze(pred_mean)-2*np.sqrt(pred_var), \n",
    "                    np.squeeze(pred_mean)+2*np.sqrt(pred_var), alpha=0.2, color='blue')\n",
    "    ax.plot(x_train, y_train, 'or')\n",
    "    ax.plot(x_plot, y_plot, 'black')\n",
    "    \n",
    "    for i in range(funcs.shape[0]):\n",
    "        # plot function samples\n",
    "        ax.plot(x_plot, funcs[i], 'red', alpha=0.4)\n",
    "    \n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5) Discussion (3 Points)\n",
    "In the plots above we visualized the predictive distribution and sample functions of BLR for different values of the hyperparameters $\\lambda$ and $\\sigma$. \n",
    "\n",
    "- Discuss how the predictive distribution/sample functions change in dependence of these hyperparameters. Explain why this is the case!\n",
    "\n",
    "- Assume you only had access to `blr_sample_function` but not to `blr_predictive_distribution`. How could you approximate the parameters of the predictive distribution using only function samples?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Gaussian Processes (5 Points)\n",
    "In this exercise, we will study the kernelized version of BLR, the Gaussian Process (GP). We will apply GPs on the same data set as before."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1) Gaussian Kernel (2 Points)\n",
    "Recall that for BLR we worked with a fixed set of features. In contrast, for GPs we define a discrepancy measure between input points, the so-called kernel function $k$, which by means of the kernel trick implicitly defines a (possibly infinite-dimensional) set of features. As we never have to explicitly evaluate these implicit features, we can work in very expressive feature spaces and still obtain a tractable algorithm. \n",
    "\n",
    "In the lecture, we defined the Gaussian kernel function as\n",
    "$$\n",
    "k(\\boldsymbol{x}, \\boldsymbol{x}') := \\lambda^{-1} \\exp\\left(-\\dfrac{||\\boldsymbol{x}-\\boldsymbol{x}'||^2}{2\\sigma^2}\\right), \\quad \\boldsymbol{x}, \\boldsymbol{x'} \\in \\mathbb R^d,\n",
    "$$\n",
    "where $\\lambda$ denotes the prior precision parameter and $\\sigma^2$ is the kernel bandwidth.\n",
    "\n",
    "Furthmerore, given training inputs $\\boldsymbol X = \\left\\{\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_N\\right\\}$, $\\boldsymbol{x}_i \\in \\mathbb R^d$, we defined the kernel vector as \n",
    "$$\n",
    "\\boldsymbol{k}(\\boldsymbol{x}) := \\left(k(\\boldsymbol{x}_1, \\boldsymbol{x}), \\dots, k(\\boldsymbol{x}_N, \\boldsymbol{x}) \\right)^T.\n",
    "$$\n",
    "Note that the dimension of the kernel vector is now determined by the number $N$ of training examples. In contrast, for BLR, the dimension of the feature vector $k$ was a fixed constant. We already discussed this distinction (parametric vs. non-parametric/instance-based methods) a number of times in the lecture.\n",
    "\n",
    "Finally, we define the *kernel matrix* as\n",
    "$$\n",
    "\\boldsymbol K := \n",
    "\\begin{pmatrix}\n",
    "  \\boldsymbol{k}(\\boldsymbol{x}_1) &  \\dots & \\boldsymbol{k}(\\boldsymbol{x}_N)\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "  k(\\boldsymbol{x}_1, \\boldsymbol{x}_1) & \\dots & k(\\boldsymbol{x}_1, \\boldsymbol{x}_N) \\\\\n",
    "  \\vdots & \\vdots & \\vdots \\\\\n",
    "  k(\\boldsymbol{x}_N, \\boldsymbol{x}_1) & \\dots & k(\\boldsymbol{x}_N, \\boldsymbol{x}_N) \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Implement the Gaussian kernel vector.\n",
    "\n",
    "**Hints**:\n",
    "- Note that you are supposed to compute the kernels for a batch of inputs $\\boldsymbol{x}$ and for a batch of inputs $\\boldsymbol{x'}$ in one function call.\n",
    "- As always, no for-loops are allowed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x: np.ndarray, x_prime: np.ndarray, lam: float, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param x: first input (shape: [N_1, d])\n",
    "    :param x_prime: second input (shape: [N_2 x d])\n",
    "    :param lam: prior precision parameter (scalar)\n",
    "    :param sigma: bandwidth of the kernel (scalar)\n",
    "    :return: the Gaussian kernel, evaluated at all pairs (x, x') (shape: [N_1 x N_2])\n",
    "    \"\"\"\n",
    "    if len(x.shape) == 1:\n",
    "        x = x.reshape((-1, 1))\n",
    "        \n",
    "    if len(x_prime.shape) == 1:\n",
    "        x_prime = x_prime.reshape((-1, 1))\n",
    "        \n",
    "    ############################################################\n",
    "    # TODO Implement the Gaussian kernel\n",
    "    dist_squared = np.sum((x[:, None] - x_prime[None, :]) ** 2, axis=-1)\n",
    "    kernel = lam ** (-1) * np.exp(-dist_squared / (2 * sigma ** 2))\n",
    "    ############################################################\n",
    "    \n",
    "    assert kernel.shape == (x.shape[0], x_prime.shape[0])\n",
    "    return kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `gaussian_kernel`, we can now easily compute the Gaussian kernel matrix. You do not need to implement anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_matrix(X: np.ndarray, lam: float, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param X: training data (shape: [N, d])\n",
    "    :param lam: prior precision parameter (scalar)\n",
    "    :param sigma: bandwidth of the kernel (scalar)\n",
    "    :return: the kernel matrix (N_train x N_train)\n",
    "    \"\"\"\n",
    "    return gaussian_kernel(x=X, x_prime=X, lam=lam, sigma=sigma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2) Predictive Distribution for GPs (2 Points)\n",
    "From the lecture, we know the predictive mean and variance at a query input $\\boldsymbol{x}^* \\in \\mathbb R^d$ for GPs read:\n",
    "$$\n",
    "\\mu(\\boldsymbol{x}^*) = \\boldsymbol{k}(\\boldsymbol{x}^*)^T \\left(\\boldsymbol K + \\sigma_y^2 \\boldsymbol I \\right)^{-1} \\boldsymbol y, \\quad \\sigma(\\boldsymbol{x}^*) = \\sigma_y^2 + k(\\boldsymbol{x}^*, \\boldsymbol{x}^*) - \\boldsymbol{k}(\\boldsymbol{x}^*)^T \\left(\\boldsymbol K + \\sigma_y^2 \\boldsymbol I \\right)^{-1} \\boldsymbol{k}(\\boldsymbol{x}^*),\n",
    "$$\n",
    "where $\\boldsymbol{y} := \\left(y_1, \\dots, y_N \\right)^T$ are the targets corresponding to $\\boldsymbol X$ and $\\sigma_y^2$ is the variance of the likelihood.\n",
    "\n",
    "Implement the predictive distribution for GPs.\n",
    "\n",
    "**Hints:** \n",
    "- We defined the likelihood variance $\\sigma_y^2$ as a global variable at the beginning of this notebook and fixed it to the true noise variance of 1.0, which we assume to be known for this exercise. \n",
    "- Avoid duplicate or unneccessary computations, and use numerically stable operations instead of computing inverses, if possible!\n",
    "- Make use of both `gaussian_kernel` and `gaussian_kernel_matrix`!\n",
    "- Do not use any for loops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_predictive_distribution(x: np.ndarray, y: np.ndarray, X: np.ndarray, lam: float, sigma: float):\n",
    "    \"\"\"\"\n",
    "    :param x: query inputs (shape: [N_q, d])\n",
    "    :param X: training inputs (shape: [N, d])\n",
    "    :param y: training targets (shape: [N, 1])\n",
    "    :param lam: prior precision parameter (scalar)\n",
    "    :param sigma: bandwidth of the kernel (scalar)\n",
    "    :return: the mean (shape: [N_q])\n",
    "             the variance (shape: [N_q])\n",
    "             of the predictive distribution\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO Implement the predictive distribution for GPs\n",
    "    K = gaussian_kernel_matrix(X, lam, sigma)\n",
    "    k_x = gaussian_kernel(X, x, lam, sigma)\n",
    "    mean_x = (k_x.T @ np.linalg.inv(K + sigma_y ** 2 * np.eye(K.shape[0])) @ y).squeeze()\n",
    "    var_x = sigma_y ** 2 + np.diag(gaussian_kernel(x, x, lam, sigma)) - np.einsum('ij,ji->i', k_x.T @ np.linalg.inv(K + sigma_y ** 2 * np.eye(K.shape[0])), k_x)\n",
    "    #var_x = sigma_y ** 2 + gaussian_kernel(x, x, lam, sigma) - k_x.T @ np.linalg.inv(K + sigma_y ** 2 * np.eye(K.shape[0])) @ k_x\n",
    "    ############################################################\n",
    "    print(var_x.shape , (x.shape[0],))\n",
    "    assert mean_x.shape == (x.shape[0],)\n",
    "    assert var_x.shape == (x.shape[0],)    \n",
    "    return mean_x, var_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us again plot the predictive distribution for different values of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lambdas and sigmas\n",
    "lambdas = [1e-5, 1e-3, 1.0, 10.0]\n",
    "feature_sigmas = [0.1, 0.5, 1.0, 10.0]\n",
    "\n",
    "# compute predictive distribution and function samples for the lambdas and sigmas\n",
    "pred_means, pred_vars, labels = [], [], []\n",
    "for lam, feature_sigma in product(lambdas, feature_sigmas):\n",
    "    # obtain the predictive distribution\n",
    "    pred_mean, pred_var = gp_predictive_distribution(x=x_plot, X=x_train, y=y_train, lam=lam, sigma=feature_sigma)\n",
    "        \n",
    "    # collect computed stuff\n",
    "    pred_means.append(pred_mean)\n",
    "    pred_vars.append(pred_var)\n",
    "    labels.append(f\"$\\lambda$ = {lam}, $\\sigma$ = {feature_sigma}\")\n",
    "    \n",
    "\n",
    "# plot \n",
    "fig, axes = plt.subplots(nrows=len(lambdas), ncols=len(feature_sigmas), sharex=True, figsize=(len(feature_sigmas)*5, len(lambdas)*3), squeeze=False)\n",
    "for i, (pred_mean, pred_var, funcs, label) in enumerate(zip(pred_means, pred_vars, funcss, labels)):\n",
    "    ax = axes[i//len(feature_sigmas), i%len(feature_sigmas)]\n",
    "    \n",
    "    # the predictive distribution together with the 95% confidence interval\n",
    "    ax.plot(x_plot, pred_mean, 'b', label=label)\n",
    "    ax.fill_between(np.squeeze(x_plot), np.squeeze(pred_mean)-2*np.sqrt(pred_var), \n",
    "                    np.squeeze(pred_mean)+2*np.sqrt(pred_var), alpha=0.2, color='blue')\n",
    "    ax.plot(x_train, y_train, 'or')\n",
    "    ax.plot(x_plot, y_plot, 'black')\n",
    "    \n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3) Sampling Functions from a GP (1 Point)\n",
    "For BLR, we could obtain function samples, evaluated at query inputs, by sampling weights from the posterior and transforming them to function evaluations at the query inputs (cf. Exercise 1.4). Explain how this works for GPs! I.e., how we can obtain function samples, evaluated at query inputs, from GPs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
