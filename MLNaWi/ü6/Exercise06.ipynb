{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ff98ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87b1431e25aa9fa7e9ea168152b7a436",
     "grade": false,
     "grade_id": "cell-6fa1bcf98952e899",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Exercise Sheet No. 6\n",
    "\n",
    "---\n",
    "\n",
    "> Machine Learning for Natural Sciences, Summer 2023, Jun.-Prof. Pascal Friederich, pascal.friederich@kit.edu\n",
    "> \n",
    "> Deadline: June 5th 2023, 8:00 am\n",
    ">\n",
    "> Container version 1.0.1\n",
    ">\n",
    "> Tutor: patrick.reiser@kit.edu\n",
    ">\n",
    "> **Please ask questions in the forum/discussion board and only contact the Tutor when there are issues with the grading**\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "**Topic**: This exercise sheet will focus on feed-forward neural networks, their implementation and training, as well as an application to materials science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44af09",
   "metadata": {},
   "source": [
    "Please add here your group members' names and student IDs. \n",
    "\n",
    "Names: Robin Maurer, Francisca Azocar Dannemann, Marcus Fledler\n",
    "\n",
    "IDs: 2462304, 2480646, 2494460"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c2dcf0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9fc6c67a5b72b907a3c10371fa0e5abe",
     "grade": false,
     "grade_id": "cell-3236a919b585de02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Application\n",
    "\n",
    "Next week you will start learning about applications of NNs in materials science. To give you a little insight, we will use an application in materials science already here:\n",
    "\n",
    "# Organic Solar Cells\n",
    "For organic materials to become semi-conducting, electrons must be delocalized in the molecule. For electrons to be delocalized, a high level of conjugation is necessary:  \n",
    "When single and double bonds are alternating in an organic molecule, electrons can move. When we think about an aromatic ring, like benzene, it is not defined where the double bonds would form, so they can move around the ring and are delocalized along the whole aromatic system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef57e56",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef4251792d2543a6430fb65b29792637",
     "grade": false,
     "grade_id": "cell-1cd6d0e1688214d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "mol = Chem.MolFromSmiles('c1ccccc1')\n",
    "mol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a012e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "914c87520a2072c223eab86a4b8cc844",
     "grade": false,
     "grade_id": "cell-ad8bd72c4fa018c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "These electrons have higher energies and are part of what is called the highest occupied molecular orbit (HOMO). This is basically the equivalent of the valence band in classical semiconductors.  \n",
    "\n",
    "The equivalent of the conduction band is the lowest level at higher energies that is unoccupied or the lowest unoccupied molecular orbit (LUMO). The gap between those two levels is the bandgap of organic semiconductors.\n",
    "\n",
    "For organic photovoltaic cells this bandgap needs to be small enough so that visible light can excite an electron from the HOMO to the LUMO. This requires a high level of conjugation and hence aromatic systems (Figure 1)\n",
    "\n",
    "<a title=\"Alevina89, CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Homo-lumo_gap.tif\"><img width=\"512\" alt=\"Homo-lumo gap\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/58/Homo-lumo_gap.tif/lossless-page1-607px-Homo-lumo_gap.tif.png\"></a>\n",
    "<p style=\"text-align:center;font-size:80%;font-style:italic\">\n",
    "    Figure 1: Conjugation induced LUMO reduction.\n",
    "</p>\n",
    "\n",
    "For the development of organic semiconductors, HOMO and LUMO can be simulated by density functional theory (DFT) using different levels of theory. You will learn about this in later lectures. All you need to know now, is that depending on the level of theory  - or better the details of the approximations taken - the calculation of properties like HOMO and LUMO from the molecule can take hours.\n",
    "\n",
    "This is a problem for high-throughput screening if one wants to discover new materials. It is extremely costly to evaluate e.g. 100,000 molecules for their properties with methods that are precise enough. Hence, one usually tries to do a detailed simulation only on a subset of molecules and then train a ML-model to predict the properties of interest on the labeled data.\n",
    "\n",
    "## Dataset\n",
    "The dataset contains the simulated LUMO and HOMO values for 51,247 organic molecules. Additionally, it contains 63 molecule descriptors that were calculated using [rdkit](https://www.rdkit.org/docs/index.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c65cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08f95d1380082c2200df08828984c752",
     "grade": false,
     "grade_id": "cell-868d8be608b5077e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_hdf('OPV.h5')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b54e5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "633636d6cac62ef0484bb5972c74ef3d",
     "grade": false,
     "grade_id": "cell-42ef56ae22c7077a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The dataframe has a two-level column index so you can index the labels by calling `df['labels']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754e44c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d0e91dfe3c2384ab4e53d11aa8dbdeb",
     "grade": false,
     "grade_id": "cell-996e09cffe1ea828",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df['labels'].hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4185002b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a5a3615b53471b9b911b0ddb1e7641d",
     "grade": false,
     "grade_id": "cell-cff397671bc615ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Linear regression benchmark\n",
    "For a first shot we can train a ridge regression to predict the `labels` from the `mol_descriptors`.\n",
    "\n",
    "This time we will use `sklearn`. Additionally we will also pre-process the features with a standard scaler to shift them to zero mean and scale them to unit variance.\n",
    "\n",
    "As already mentioned, the calculation of DFT properties can be very costly. Hence, we want to have a model that extrapolates well to unseen data. Hence, we will use only 20% of the dataset for training and test on 80%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849afb54",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4144c4650fe99794fb819743ad51fc90",
     "grade": false,
     "grade_id": "cell-200cc37a5b72eb23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = df['mol_descriptors'].values\n",
    "y = df['labels'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55be8b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebbd6b8923aa3df805a0742916705d33",
     "grade": false,
     "grade_id": "cell-5c747b57ec6724fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First use the [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to scale the features (`X`) to mean of 0 and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5ceec0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eea93bf92b9f5d8b68c9c2c751580c46",
     "grade": false,
     "grade_id": "cell-c9f2df20a8d0cf01",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assign a StandardScaler object to scaler and obtain the scaled fatures as X_scaled\n",
    "scaler = None\n",
    "X_scaled = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f45abc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c8a5ab885363529eebab69020604548",
     "grade": true,
     "grade_id": "Scaler-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Scaler - 1 point\n",
    "\n",
    "assert isinstance(scaler, StandardScaler), \"The scaler should be an instance of the sklearn StandardScaler\"\n",
    "np.testing.assert_almost_equal(np.mean(X_scaled), 0, 10)\n",
    "np.testing.assert_almost_equal(np.var(X_scaled, axis=0).sum(), 63.)\n",
    "# Possible hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f344e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c3b0c0a2f1b58b77004de8fae20133b",
     "grade": false,
     "grade_id": "cell-d0ed310e0c9ef260",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next we use the [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn.model_selection.train_test_split) to split of 20% of `X_scaled` and `y` as train set and use the rest as test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f105ae7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a78eed9979bc872c50db84a260c8a5c",
     "grade": false,
     "grade_id": "cell-6e293b67656d2a39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f325e320",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b29110c13e1d3ff7772eed7a5d1796a4",
     "grade": false,
     "grade_id": "cell-eabac20dab75b13d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now use the [`Ridge()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html?highlight=ridge#sklearn.linear_model.Ridge) model to fit a ridge regression with standard parameters to the train data, and assign the predictions of the fitted model on the test data to `y_pred`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04d5ff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e3006652180e3013f24e9b708880ca4",
     "grade": false,
     "grade_id": "cell-b1b48084492b70d2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a Ridge() model as model, fit it and assign the predictions to y_pred\n",
    "model = None\n",
    "y_pred = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "model = Ridge()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35349e16",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e122ed242136664f92b33a73a71321f0",
     "grade": false,
     "grade_id": "cell-69dfde6ff482f06b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "r2_lumo_ridge = r2_score(y_test[:, 0], y_pred[:, 0])\n",
    "r2_homo_ridge = r2_score(y_test[:, 1], y_pred[:, 1])\n",
    "print(f'R2 LUMO: {r2_lumo_ridge}\\nR2 HOMO: {r2_homo_ridge}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2829c26",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "438eb3187e157479740bea551785acab",
     "grade": true,
     "grade_id": "Ridge_Regression-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Ridge Regression - 1 point\n",
    "\n",
    "assert y_pred.shape[0] == y_test.shape[0]\n",
    "assert y_pred.shape[1] == y_test.shape[1]\n",
    "assert r2_lumo_ridge > 0.70\n",
    "assert r2_homo_ridge > 0.78\n",
    "# Possible hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57903b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f58d0ad84f10801c84341ddf6c449e2",
     "grade": false,
     "grade_id": "cell-372ca63030fe0b3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can plot the correlations of the the true and predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7311a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "356572c32d7c3285dcb85abbc39f2542",
     "grade": false,
     "grade_id": "cell-0a06841aa21a0f40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axs[0].scatter(y_test[:, 0], y_pred[:, 0], alpha=0.2, s=5)\n",
    "axs[0].plot([-6, -1], [-6, -1], 'k')\n",
    "axs[0].set_title(f'R2 LUMO: {r2_lumo_ridge}')\n",
    "axs[0].set_xlabel('true LUMO')\n",
    "axs[0].set_ylabel('predicted LUMO')\n",
    "axs[0].set_xlim([-6, -1])\n",
    "axs[0].set_ylim([-6, -1])\n",
    "\n",
    "axs[1].scatter(y_test[:, 1], y_pred[:, 1], alpha=0.2, s=5)\n",
    "axs[1].plot([-9, -3], [-9, -3], 'k')\n",
    "axs[1].set_title(f'R2 HOMO: {r2_homo_ridge}')\n",
    "axs[1].set_xlabel('true HOMO')\n",
    "axs[1].set_ylabel('predicted HOMO')\n",
    "axs[1].set_xlim([-9, -3])\n",
    "axs[1].set_ylim([-9, -3])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c52d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f47be03e3b8b9db3a3c4ce79daa55bd1",
     "grade": false,
     "grade_id": "cell-b045f7ffe8f279f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, the ridge regression learns something, but the predictions are still way off from the true values.\n",
    "\n",
    "Hence, we will apply a non-linear model, namely a:\n",
    "\n",
    "# Neural Network\n",
    "\n",
    "## Inspiration from Neuroscience\n",
    "In this week's lecture you saw that feedforward neural networks can be thought of as 'function approximation machines' [Goodfellow et al. 2016](https://www.deeplearningbook.org/contents/mlp.html).\n",
    "Some ideas of todays artificial neural networks are loosely inspired by neuroscientific models of biological networks.\n",
    "For a different perspective we can try to make a connection to models of biological neural networks (while remembering the goal of todays neural networks is not to get a model of the brain but rather to approximate some function).\n",
    "\n",
    "The fundamental units of the brain are neurons which can also be thought of as information messengers. Neurons receive, transmit and transform information in the form of electrical impulses.\n",
    "A neuron integrates information from its upstream neurons at the dendrites, creating a post-synaptic potential. \n",
    "At the axon hillock this potential is encoded into a spike train of action potentials. These are sent down the axon until they reach a synapse of an axon terminal. At the synapse the spike train is again decoded into a pre-synaptic potential and depending on the incoming spike frequency more or less neurotransmitters are released to pass the signal on to the next neuron (Figure 2).\n",
    "<p style=\"text-align:center;font-size:80%;font-style:italic\">\n",
    "    <img src=\"https://images.topperlearning.com/topper/tinymce/imagemanager/files/Synapse_between_Neurons.jpg\", width=\"70%\">\n",
    "    <br>\n",
    "    Figure 2: Simplified sketch of a neuron.\n",
    "</p>\n",
    "\n",
    "\n",
    "## Forward Pass\n",
    "\n",
    "For todays artificial neural networks this was simplified in the following way:  \n",
    "Instead of simulating spikes, we only simulate a spike frequency as a continuous value. Depending on the post-synaptic potential, this frequency can be higher or lower. E.g. neurons do not respond at all until a specific post-synaptic potential is reached and also have a maximum spike-frequency. This implies two things:  \n",
    "**1. The transformation from post-synaptic potential to spike-frequency is non-linear. (activation function)**  \n",
    "**2. Each neuron has a different base activity. (bias)**\n",
    "\n",
    "In the following mathematical definitions, lower-case letters denote scalars, while upper-case letters denote vectors or matrices. No dot or $\\cdot$ denotes a dot product of vectors or matrices.\n",
    "\n",
    "### Single neuron\n",
    "Given a subjected neuron $l$ receives inputs from an upstream layer $k$ and the upstream layer consists of three neurons with the outputs $x_1, x_2, x_3$ via connections with weights $\\theta_{1,l}, \\theta_{2,l}, \\theta_{3,l}$ (Figure 3), we can compute the weighted sum as state $h_l(X)$ of the subjected neuron by linear algebra as the product of the row vector $X_k$ and the column vector $\\Theta_{k,l}$. Additionally, we add a bias $b$ to the neuron.\n",
    "\n",
    "\\begin{align}\n",
    "h_l(X) &= X_k \\cdot \\Theta_{k,l} + b_l\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    x_1 & x_2 & x_3\n",
    "\\end{bmatrix} \n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "    \\theta_{1,l} \\\\ \\theta_{2,l} \\\\ \\theta_{3,l}\n",
    "\\end{bmatrix}\n",
    "+ b_l\\\\\n",
    "&=x_1 \\theta_{1,l} +  x_2 \\theta_{2,l} + x_3 \\theta_{3,l} + b_l\n",
    "\\end{align}\n",
    "\n",
    "And the output or activation $a_l(x)$ of the subjected neuron in terms of spike-frequency is computed using a non-linear activation function $\\sigma()$:\n",
    "\n",
    "\\begin{align}\n",
    "a_l(X) &= \\sigma(h_l(X))\n",
    "\\end{align}\n",
    "\n",
    "<div>\n",
    "<img src=\"attachment:ff_b1.png\" width=\"30%\"/>\n",
    "</div>\n",
    "<p style=\"text-align:center;font-size:80%;font-style:italic\">\n",
    "Figure 3: A neuron layer k with three neurons has feed-forward connections to a single neuron l.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53f355",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b73b00204c130c25d426c5b774fd2c1",
     "grade": false,
     "grade_id": "cell-65de2b486b4e7331",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Neuron Layer\n",
    "Now given that the subjected $l$ is not only a single neuron but e.g. a layer of two neurons (Figure 4), the notation is the same with the only difference that we add a column to the weight column vector $\\Theta_{k,l}$ changing it to a $3 \\times 2$ dimensional matrix to calculate the $1 \\times 2$ dimensional row state vector $H_l(X)$:\n",
    "\n",
    "\\begin{align}\n",
    "    H_l(X) &= X_k \\cdot \\Theta_{k,l} + B_l\\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "        x_1 & x_2 & x_3\n",
    "    \\end{bmatrix} \n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "        \\theta_{1,1} & \\theta_{1,2} \\\\\n",
    "        \\theta_{2,1} & \\theta_{2,2} \\\\\n",
    "        \\theta_{3,1} & \\theta_{3,2} \n",
    "    \\end{bmatrix}\n",
    "    +\n",
    "    \\begin{bmatrix}\n",
    "        b_1 & b_2 \n",
    "    \\end{bmatrix} \\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "        \\theta_{1,1} x_1 + \\theta_{1,2} x_2 + \\theta_{1,3} x_3 + b_1 &\n",
    "        \\theta_{2,1} x_1 + \\theta_{2,2} x_2 + \\theta_{2,3} x_3 + b_2\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "And the activation of the layer:\n",
    "\n",
    "\\begin{align}\n",
    "A_i(X) &= \\sigma(H_i(X))\n",
    "\\end{align}\n",
    "\n",
    "<div>\n",
    "<img src=\"attachment:ff_b2.png\" width=\"30%\"/>\n",
    "</div>\n",
    "<p style=\"text-align:center;font-size:80%;font-style:italic\">\n",
    "Figure 4: A neuron layer j with three neurons has feed-forward connections to a neuron layer i with two neurons.\n",
    "</p>\n",
    "\n",
    "For the bias there exist two approaches:\n",
    "1. One can see the bias as an additional input with a constant $1$ from the previous layer and a learnable weight vector.\n",
    "2. Adding the bias simply as row vector that added to the neuron state $H(x)$.\n",
    "\n",
    "Here we use approach 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bafbab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "077dafa4e12dc6c4e8af45f09d1543e3",
     "grade": false,
     "grade_id": "cell-635ae832ad50c9c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Batches\n",
    "Usually data is processed in batches, so not only one single sample is processed at a time but multiple samples as this is much faster when calculated using vectorization. In our case, samples are rows with the length of the features. So when we pass 10 samples of our features (the `mol_descriptors`), we pass a matrix of shape $10 \\times 63$. This already fits well with our current notation and it will return a $10 \\times 32$ state matrix.\n",
    "\n",
    "\n",
    "### Weight initialization\n",
    "To compute anything, the weights must be different from $0$. There are several approaches to initialize weights. We will use the approach of [Glorot et Al., 2010](http://proceedings.mlr.press/v9/glorot10a.html).\n",
    "The initial weights are drawn form a uniform distribution $U(-z, z)$ with the limit $z$ calculated as:\n",
    "\\begin{align}\n",
    "    z &= \\sqrt{\\frac{6}{k+l}}\n",
    "\\end{align}\n",
    "...with input length $k$ and output length $l$.\n",
    "\n",
    "\n",
    "Just to get an idea how this all works in python we will build a layer with 32 neurons and feed all `mol_descriptors`.  \n",
    "Initialize the weight matrix `theta` using numpy functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53593e5c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fbecf8fb397c63e44ce1eb5bffa3f56",
     "grade": false,
     "grade_id": "cell-6da9b09109577066",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sample = X_train[0:10]\n",
    "n_inputs = sample.shape[1]\n",
    "n_outputs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d3a05",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e12c0498043d82f9f0df88e867892a89",
     "grade": false,
     "grade_id": "cell-31a3010d3f198150",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the weights theta after Glorot et Al. 2010 (glorot uniform):\n",
    "theta = None\n",
    "# YOUR CODE HERE\n",
    "Z=np.sqrt(6/(n_inputs+n_outputs))\n",
    "theta = np.random.uniform(-Z,Z,size=(n_inputs,n_outputs))\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d035e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "249e1c571db8c4153d3bfd53d467a62e",
     "grade": true,
     "grade_id": "Weights-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Weights - 1 point\n",
    "\n",
    "assert theta.shape[0] == n_inputs, \"Your weight shape doesn't match!\"\n",
    "assert theta.shape[1] == n_outputs, \"Your weight shape doesn't match!\"\n",
    "\n",
    "# Hidden asserts for the limits of the uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36477069",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a8d40b239a972514487a815cce117a4",
     "grade": false,
     "grade_id": "cell-6f705f2c4c9570d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now calculate h(x) for the sample using your weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62668620",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b6f77c2eaeb20c4ad68cd763fa03328",
     "grade": false,
     "grade_id": "cell-e47a93a7f3cc2876",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the bias row vector with zeros:\n",
    "b = None\n",
    "# YOUR CODE HERE\n",
    "b = np.zeros((n_outputs))\n",
    "#raise NotImplementedError()\n",
    "\n",
    "# Calculate h(x) using sample, theta and b\n",
    "# The bias won't change anything but it will check for correct shapes.\n",
    "h = None\n",
    "# YOUR CODE HERE\n",
    "h = sample@theta+b\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47b96c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dc5b9220c0b0d8f8f843afe4da33f68",
     "grade": true,
     "grade_id": "States-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# States - 2 points\n",
    "\n",
    "assert h.shape[0] == sample.shape[0]\n",
    "assert h.shape[1] == n_outputs\n",
    "# Possible hidden asserts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8aec0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53548aa2148e92a3ccbca80706782f95",
     "grade": false,
     "grade_id": "cell-1b37830e571f872c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Activation functions\n",
    "\n",
    "Our model will be a regression model. For the output of the model we need to fit the unscaled `y`. Hence we need an unbound linear activation function for the output:\n",
    "\n",
    "\\begin{align}\n",
    "    linear(H(x)) &= H(x)\n",
    "\\end{align}\n",
    "\n",
    "As non-linear activation function for the hidden layers we will use the ReLu function:\n",
    "\n",
    "\\begin{align}\n",
    "    relu(H(x)) &= \n",
    "\\begin{cases} \n",
    "    0~\\text{ if }~h_l(x) \\leq 0\\\\\n",
    "    h_l(x)~\\text{ if }~h_l(x)>0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "The ReLu has the biological motivation of having a minimal threshold until it starts outputting values different from 0. Additionally, it yields sparse activations in the network, which is usually favorable, and its gradient is trivial to calculate.\n",
    "\n",
    "Please calculate the ReLu of your previously calculated `h`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b60bed",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ab1acb5fc88c13ad757486c80348314",
     "grade": false,
     "grade_id": "cell-5c8a3e28ebedb119",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the activation a by computing the ReLu of h\n",
    "a = None\n",
    "# YOUR CODE HERE\n",
    "a = np.maximum(0,h) \n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63213da8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b6cf6d2a764b2c0113a0678acb0e9cf",
     "grade": true,
     "grade_id": "ReLu-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ReLu - 1 point\n",
    "\n",
    "assert np.min(a) >= 0\n",
    "# Possible hidden asserts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf54d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a6a2a28475def9fb6d582bd318b32b0",
     "grade": false,
     "grade_id": "cell-b2b2af32ce0ca084",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "### Weights\n",
    "From the lecture recall the following chain to get the gradient of the error $J$ with respect to the weights $\\Theta$ of the output layer:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial \\Theta} &= \\underset{(1)}{\\frac{\\partial J}{\\partial a}} \n",
    "                                      \\underset{(2)}{\\frac{\\partial a}{\\partial h}}\n",
    "                                      \\underset{(3)}{\\frac{\\partial h}{\\partial \\Theta}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "For our case the error is the MSE defined as:\n",
    "\\begin{align}\n",
    "J &= \\frac{1}{2m} \\sum_{i=1}^m \\left( a^{(i)} - y^{(i)} \\right)^2\n",
    "\\end{align}\n",
    "...for $m$ samples $i$.\n",
    "\n",
    "Everything else is given. Now calculate the partial derivatives for a single sample $i$ of:  \n",
    "**(1) the MSE  \n",
    "(2) the linear and ReLu functions  \n",
    "(3) $h(x)$**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ce988",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "650a0f838bd12eee53c46634e7114de0",
     "grade": false,
     "grade_id": "cell-c639ca9b3959931f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(r\"\\begin{align}\"\n",
    "                 r\"\\frac{\\partial \\text{MSE}}{\\partial a^{(i)}} &= \\frac{1}{m} \\left( a^{(i)} - y^{(i)} \\right) \\tag{1}\\\\\"\n",
    "                 r\"\\frac{\\partial \\text{ReLu}^{(i)}}{\\partial h^{(i)}} &= \"\n",
    "                 r\"\\begin{cases} \"\n",
    "                 r\"0~\\text{ if }~h^{(i)}(x) \\leq 0\\\\\"\n",
    "                 r\"1~\\text{ if }~h^{(i)}(x)>0\"\n",
    "                 r\"\\end{cases} \\tag{2}\\\\\"\n",
    "                 r\"\\frac{\\partial \\text{linear}^{(i)}}{\\partial h^{(i)}} &= 1 \\tag{2}\\\\\"\n",
    "                 r\"\\frac{\\partial h^{(i)}}{\\partial \\theta^{(i)}} &= x^{(i)} \\tag{3}\"\n",
    "                 r\"\\end{align}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b9d45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ab2acada8e49cf1b058b83d633c227a",
     "grade": false,
     "grade_id": "cell-c47108b07d99fbf0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's take the above calculated `a` as output and calculate some random true labels `y_` for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc3429",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "984ef23cfd5fda32d981d71af0c46445",
     "grade": false,
     "grade_id": "cell-1b8e39a0adf8c3a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_ = np.random.uniform(low=0, high=3, size=[10, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a75db2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "422b66439135a32895755ec4840a7dfd",
     "grade": false,
     "grade_id": "cell-00bda07a4a44647d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next we can use the derivatives from above to calculate the gradient for one layer:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial \\Theta} &= \\underset{(1)}{\\frac{\\partial J}{\\partial a}} \n",
    "                                      \\underset{(2)}{\\frac{\\partial a}{\\partial h}}\n",
    "                                      \\underset{(3)}{\\frac{\\partial h}{\\partial \\Theta}} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d5982",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92779ac0363dc94a79abde8b4453a8f7",
     "grade": false,
     "grade_id": "cell-d04d29795455d3b4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We start from the back:\n",
    "# (3) is trivial\n",
    "dh_dtheta = None\n",
    "# YOUR CODE HERE\n",
    "dh_dtheta=sample\n",
    "#raise NotImplementedError()\n",
    "\n",
    "# (2) for ReLu: Here you need a reference to h and maybe two steps\n",
    "da_dh = None\n",
    "# YOUR CODE HERE\n",
    "da_dh = np.where(h < 0, 0, 1)\n",
    "#raise NotImplementedError()\n",
    "\n",
    "# (1) m is the batch size!\n",
    "dJ_da = None\n",
    "# YOUR CODE HERE\n",
    "dJ_da = (1/len(sample))*(a-y_)\n",
    "#raise NotImplementedError()\n",
    "\n",
    "dJ_dTheta = np.dot(dh_dtheta.T, (dJ_da * da_dh))\n",
    "# The error gradient with respect to the weights and the shape of the weights should agree:\n",
    "print(dJ_dTheta.shape, theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b87c0db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4234c64dbac518d3e8e61520d3f745b2",
     "grade": true,
     "grade_id": "dh_dtheta-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# dh dtheta - 1 point\n",
    "\n",
    "assert dh_dtheta.shape[0] == 10\n",
    "assert dh_dtheta.shape[1] == 63\n",
    "# Hidden test for the content of dh_dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd333169",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d11c554e8aecdf1be6ebfc92d4f1637a",
     "grade": true,
     "grade_id": "da_dh-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# da dh - 1 point\n",
    "\n",
    "assert da_dh.shape[0] == 10\n",
    "assert da_dh.shape[1] == 32\n",
    "# Hidden test for the content of drelu_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d23279",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bee76a01284e330535afe1d7ba5b6d95",
     "grade": true,
     "grade_id": "dJ_da-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# dJ dh - 1 point\n",
    "\n",
    "assert dJ_da.shape[0] == 10\n",
    "assert dJ_da.shape[1] == 32\n",
    "# Hidden test for the content of dJ_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ebadf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3b2f296fac8196ac43a71c7f3b13a99",
     "grade": false,
     "grade_id": "cell-ba0b36989b2f7672",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can then update the weights for the next step using:\n",
    "\\begin{align}\n",
    "    \\Theta_{t+1} &= \\Theta_t - \\alpha \\cdot \\frac{\\partial J}{\\partial \\Theta} \\\\\n",
    "\\end{align}\n",
    "...with learning rate $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c8e2f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a723e4505dbb32f796f5f0d187891f0",
     "grade": false,
     "grade_id": "cell-b7e8e14e2d041ed8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Bias \n",
    "\n",
    "Besides the weights we also need to fit the bias. The bias can be derived in the same manner, only that instead of $x$ the input is $1$. We simply multiply $1$ with the bias weights. Therefore (3) only for the bias collapses to:\n",
    "\\begin{align}\n",
    "\\frac{\\partial h^{(i)}_b}{\\partial \\theta^{(i)}_b} &= x^{(i)}_b = 1\\tag{3}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f8ac2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72d713aef4e8a41b69704987b4972425",
     "grade": false,
     "grade_id": "cell-25e02e4c3321aa4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next we will build a simple feed forward network.\n",
    "\n",
    "Recall from the lecture (Figure 5):\n",
    "\n",
    "<div>\n",
    "<img src=\"attachment:backprop_1.png\" width=\"80%\"/>\n",
    "</div>\n",
    "<p style=\"text-align:center;font-size:80%;font-style:italic\">\n",
    "Figure 5: Backpropagation.\n",
    "</p>\n",
    "\n",
    "For backpropagation we are still missing the one element connecting the layers, namely $\\frac{\\partial a^2}{\\partial a^1}$:\n",
    "\\begin{align}\n",
    "a^2 &= \\Theta^2 a^1 \\\\\n",
    "\\frac{\\partial a^2}{\\partial a^1} &= \\Theta^2\n",
    "\\end{align}\n",
    "\n",
    "With this and the above derived equations we can calculated the gradient of the first (hidden) layer weights regarding the output error as:\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial \\Theta^1} &= \\underbrace{\\frac{\\partial J}{\\partial a^2}\n",
    "                                                    \\frac{\\partial a^2}{\\partial a^1}}_\\text{(a.)}\n",
    "                                        \\underbrace{\\frac{\\partial a^1}{\\partial h^1}\n",
    "                                                    \\frac{\\partial h^1}{\\partial \\Theta^1}}_\\text{(b.)} \\\\\n",
    "\\end{align}\n",
    "**The part (a.) can be calculated in the second (output) layer and is returned as upstream gradient.**\n",
    "\n",
    "**In the first (hidden) layer we can then use this gradient and combine it with part (b.) to obtain the gradient for the weights of layer 1 with respect to the output error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b7d07",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8074640cf9b5e7032a04ff486ef892db",
     "grade": false,
     "grade_id": "cell-336b1a80f598e3c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's put this into work by building a feed forward network for our regression task with:\n",
    "\n",
    "1. The 63 `mol_descriptors` as input $x^{(i)}$\n",
    "2. One hidden layer $1$ with 32 neurons and a ReLu activation: `class HiddenLayer`\n",
    "3. One output layer $2$ with two outputs (LUMO and HOMO) with a linear (no) activation function: `class OutputLayer`\n",
    "\n",
    "Both have a forward pass, which calculates the output of the layer, a backward pass which calculates the gradients and an update function that updates the weights using the gradients and the learning rate.\n",
    "\n",
    "Only the `OutputLayer` has to return the part (a.) from above to then pass it on to the backward pass of the `HiddenLayer`.\n",
    "\n",
    "We start implementing from the bottom up with the `OutputLayer` (layer 2):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59825b2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be9b4aba1c94e576bb2071f20a48c15e",
     "grade": false,
     "grade_id": "cell-5cec7f87a6783db2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class OutputLayer:\n",
    "\n",
    "    def __init__(self, n_inputs: int, n_outputs: int):\n",
    "        # Initialize (n_inputs X n_outputs)-dimensional weight matrix self.theta with a\n",
    "        # Glorot et Al. 2010 uniform initialization:\n",
    "        self.theta = None\n",
    "        # YOUR CODE HERE\n",
    "        Z=np.sqrt(6/(n_inputs+n_outputs))\n",
    "        self.theta = np.random.uniform(-Z,Z,size=(n_inputs,n_outputs))\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        # Initialize the bias vector self.b to zeros:\n",
    "        self.b = None\n",
    "        # YOUR CODE HERE\n",
    "        self.b = np.zeros((1,n_outputs))\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        self.input = input_vector\n",
    "        # Compute the states h(x) as self.h\n",
    "        self.h = None\n",
    "        # YOUR CODE HERE\n",
    "        self.h = self.input@self.theta+self.b\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "        # As this is a linear layer a(x) = h(x)\n",
    "        self.a = self.h\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, y_predicted, y_true):\n",
    "        # HINT: as we do things backwards you might have to transpose some matrices\n",
    "\n",
    "        # partial derivative of the states with respect to the weights\n",
    "        dh2_dtheta2 = None\n",
    "        # YOUR CODE HERE\n",
    "        dh2_dtheta2 = self.input\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "        # partial derivative of activations with respect to the states\n",
    "        # One as we have a linear/no activation for the regression output\n",
    "        da2_dh2 = 1\n",
    "        \n",
    "        # Kontrolle: ich glaube hier muss keine 2 hin, deswegen habe ich sie weggemacht\n",
    "        # partial derivative of the error (MSE) with respect to the acivation\n",
    "        # infer the batch size from the input shape for normalization\n",
    "        dJ_da2 = None\n",
    "        # YOUR CODE HERE\n",
    "        dJ_da2 = (y_predicted - y_true)/y_predicted.shape[0]\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "        # Gradient of the weights with respect to the error\n",
    "        # for the weight updates for this layer:\n",
    "        self.dJ_dTheta2 = None\n",
    "        # YOUR CODE HERE\n",
    "        self.dJ_dTheta2 = np.dot(dh2_dtheta2.T, (da2_dh2*dJ_da2))\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "        # Gradient of the bias with respect to the error\n",
    "        # Recall using dh2_db2 = 1 and handle the batch size by summation\n",
    "        self.dJ_db2 = None\n",
    "        # YOUR CODE HERE\n",
    "        self.dJ_db2 = np.sum(da2_dh2 * dJ_da2, axis=0)\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "        # The downstream gradient for layer 1.\n",
    "        # employing da2_da1 = theta\n",
    "        downstream_gradient = None\n",
    "        # YOUR CODE HERE\n",
    "        downstream_gradient = np.dot(da2_dh2 * dJ_da2, self.theta.T)\n",
    "        #raise NotImplementedError()\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        # You don't need to change this\n",
    "        # HINT:\n",
    "        # If your model gets worse instead of better make sure you calculate the correct MSE\n",
    "        self.theta = self.theta - learning_rate * self.dJ_dTheta2\n",
    "        self.b = self.b - learning_rate * self.dJ_db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4c5d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01dbdad3a58c5a41b09b3aadef59de4a",
     "grade": false,
     "grade_id": "cell-a2f72c3745dfbb62",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_sample = X_train[0:100, :]\n",
    "y_sample = y_train[0:100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e45c79",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47b1bbc1e90b5ad8a1412d3dbbf97303",
     "grade": true,
     "grade_id": "Output_Forward_Pass-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Output Forward Pass - 1 point\n",
    "\n",
    "l2 = OutputLayer(X_sample.shape[1], y_sample.shape[1])\n",
    "y_pred_sample = l2.forward(X_sample)\n",
    "\n",
    "assert y_sample.shape[0] == y_pred_sample.shape[0]\n",
    "assert y_sample.shape[1] == y_pred_sample.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f8e63",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0c70551912c6b9a0e4549980fdc7477",
     "grade": true,
     "grade_id": "Output_Backward_Pass-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Output Backward Pass - 2 points\n",
    "\n",
    "downstream_gradient = l2.backward(y_pred_sample, y_sample)\n",
    "\n",
    "assert downstream_gradient.shape[0] == X_sample.shape[0]\n",
    "assert downstream_gradient.shape[1] == X_sample.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e249bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37456bb228a6b562df835641760c5e12",
     "grade": true,
     "grade_id": "Output_Weight_Update-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Output Weight Update - 2 points\n",
    "\n",
    "l2 = OutputLayer(X_sample.shape[1], y_sample.shape[1])\n",
    "y_pred_before = l2.forward(X_sample)\n",
    "\n",
    "for i in range(100):\n",
    "    y_pred_sample = l2.forward(X_sample)\n",
    "    l2.backward(y_pred_sample, y_sample)\n",
    "    l2.update(0.05)\n",
    "\n",
    "y_pred_after = l2.forward(X_sample)\n",
    "r2_before = r2_score(y_sample, y_pred_before)\n",
    "r2_after = r2_score(y_sample, y_pred_after)\n",
    "\n",
    "assert r2_before < r2_after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f690d440",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b095fdf2cfd03d569292a1962be9d662",
     "grade": false,
     "grade_id": "cell-677fcd4d04d26c49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next is the `HiddenLayer`. There are mainly two main differences you have to keep in mind here:\n",
    "1. You have to work with the ReLu activation, so $\\frac{\\partial a}{\\partial h}$ isn't $1$ anymore.\n",
    "2. We use the `upstream_gradient` for the backward pass, provided by the `backward` method of the `OutputLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea032ef",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20168340cbd6cdf2c11f9034135b25c7",
     "grade": false,
     "grade_id": "cell-10128093607d268a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class HiddenLayer:\n",
    "\n",
    "    def __init__(self, n_inputs: int, n_outputs: int):\n",
    "        # Initialize the weight matrix self.theta with a\n",
    "        # Glorot et Al. 2010 uniform initialization:\n",
    "        self.theta = None\n",
    "        # YOUR CODE HERE\n",
    "        Z=np.sqrt(6/(n_inputs+n_outputs))\n",
    "        self.theta = np.random.uniform(-Z,Z,size=(n_inputs,n_outputs))\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "        # Initialize the bias vector to zeros:\n",
    "        self.b = None\n",
    "        # YOUR CODE HERE\n",
    "        self.b = np.zeros((1,n_outputs))\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        self.input = input_vector\n",
    "        # Compute the states h(x) as self.h\n",
    "        self.h = None\n",
    "        # YOUR CODE HERE\n",
    "        self.h = self.input@self.theta+self.b\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "        # Compute the activations a(x) as self.a with ReLu activation\n",
    "        self.a = None\n",
    "        # YOUR CODE HERE\n",
    "        self.a = np.maximum(0,self.h)\n",
    "        #raise NotImplementedError()\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # HINT: as we do things backwards you might have to transpose some matrices\n",
    "\n",
    "        # Gradient of the states with respect to the inputs (trivial):\n",
    "        dh1_dtheta1 = None\n",
    "        # YOUR CODE HERE\n",
    "        dh1_dtheta1 = self.input\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        # Gradient of the activations with respect to the states:\n",
    "        # Remember to apply ReLu here\n",
    "        da1_dh1 = None\n",
    "        # YOUR CODE HERE\n",
    "        da1_dh1 =  np.where(self.h < 0, 0, 1)\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        # Gradient of the error with respect to the weights:\n",
    "        # Now we can finally use the upstream gradient...\n",
    "        self.dJ_dTheta1 = None\n",
    "        # YOUR CODE HERE\n",
    "        self.dJ_dTheta1 =np.dot(dh1_dtheta1.T, (da1_dh1*upstream_gradient))\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        # And for the bias, similar to the output layer but this time\n",
    "        # using the upstream gradient:\n",
    "        self.dJ_db1 = None\n",
    "        # YOUR CODE HERE\n",
    "        self.dJ_db1 = np.sum(da1_dh1 * upstream_gradient, axis=0)\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        # You don't need to change this\n",
    "        self.theta = self.theta - learning_rate * self.dJ_dTheta1\n",
    "        self.b = self.b - learning_rate * self.dJ_db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3962d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebf949dc6657a84671c041459ed04a50",
     "grade": false,
     "grade_id": "cell-8c9fba61d9a7e611",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We create a sample from the existing data that matches the output dimension \n",
    "# and also shift it above zero, so it is learnable with ReLu:\n",
    "\n",
    "n_hidden = 32\n",
    "\n",
    "X_sample = X_train[0:100, :]\n",
    "y_sample = np.array([y_train[i * 100:i * 100 + 100, 0] for i in range(n_hidden)]).T\n",
    "y_sample = y_sample - y_sample.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347f0bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cd3ab232551bde363a738e85c024eaa",
     "grade": true,
     "grade_id": "Hidden_Forward_Pass-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Forward Pass - 1 point\n",
    "\n",
    "l1 = HiddenLayer(X_sample.shape[1], n_hidden)\n",
    "\n",
    "y_pred_sample = l1.forward(X_sample)\n",
    "assert y_pred_sample.shape[0] == X_sample.shape[0]\n",
    "assert y_pred_sample.shape[1] == n_hidden\n",
    "assert y_pred_sample.min() >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33c3c82",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "160e25a8ac1a0fa136f6a857abd9c703",
     "grade": true,
     "grade_id": "Hidden_Weight_Update-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Weight Update - 2 points\n",
    "\n",
    "l1 = HiddenLayer(X_sample.shape[1], n_hidden)\n",
    "y_pred_before = l1.forward(X_sample)\n",
    "\n",
    "for i in range(100):\n",
    "    y_pred_sample = l1.forward(X_sample)\n",
    "    downstream_gradient = 1 / 100 * (y_pred_sample - y_sample)\n",
    "    l1.backward(downstream_gradient)\n",
    "    l1.update(0.05)\n",
    "\n",
    "y_pred_after = l1.forward(X_sample)\n",
    "r2_before = r2_score(y_sample, y_pred_before)\n",
    "r2_after = r2_score(y_sample, y_pred_after)\n",
    "\n",
    "assert r2_before < r2_after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba84fc6b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83f716735540190bf138fe343af3ca71",
     "grade": false,
     "grade_id": "cell-354dc7f5d7680449",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## NN Training\n",
    "\n",
    "Okay now let's see whether we can be better with our network than the benchmark ridge regression.\n",
    "\n",
    "As said before we will train our network in batches (`batch_size`) and for `n_epochs`.  \n",
    "Per epoch we therefore have to pass `X_train.shape[0] // batch_size` batches.  \n",
    "For each epoch we randomly shuffle the dataset. Using `np.random.permutation(X_train.shape[0])` we generate a randomly shuffled index. Indexing X and y with slices of this shuffled index, creates differently shuffled batches for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72205f2c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e9496a196e31d24b70904c2356e32c8",
     "grade": false,
     "grade_id": "cell-db017859cd6974c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_hidden = 64\n",
    "lr = 0.05\n",
    "n_epochs = 60\n",
    "batch_size = 100\n",
    "n_batches = X_train.shape[0] // batch_size\n",
    "\n",
    "l1 = HiddenLayer(X_train.shape[1], n_hidden)\n",
    "l2 = OutputLayer(n_hidden, y_train.shape[1])\n",
    "\n",
    "y_pred_before = l2.forward(l1.forward(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ccc17",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "479d999efc2dd6a968ae20950766ad2a",
     "grade": false,
     "grade_id": "cell-5e91f098bcd19cff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    permutation = np.random.permutation(X_train.shape[0])\n",
    "\n",
    "    for batch in range(n_batches):\n",
    "        start = batch * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch = X_train[permutation[start:end]]\n",
    "        y_batch = y_train[permutation[start:end]]\n",
    "\n",
    "        # Get the predictions\n",
    "        y_pred = None\n",
    "        # YOUR CODE HERE\n",
    "        y_pred = l2.forward(l1.forward(X_batch))\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        # Do the backward pass of both layers and update the weights\n",
    "        # YOUR CODE HERE\n",
    "        downstream_gradient=l2.backward(y_pred, y_batch)\n",
    "        l1.backward(downstream_gradient)\n",
    "        l2.update(0.05)\n",
    "        l1.update(0.05)\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    y_pred = l2.forward(l1.forward(X_test))\n",
    "    if epoch % 4 == 0:\n",
    "        print(f\"Epoch {epoch}: Test R2 = {r2_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08ec70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "512a47b90437c3dd95c5aa3b998738f8",
     "grade": true,
     "grade_id": "Full_Training-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Full Training - 2 points\n",
    "\n",
    "y_pred_after = l2.forward(l1.forward(X_test))\n",
    "\n",
    "r2_before = r2_score(y_test, y_pred_before)\n",
    "r2_after = r2_score(y_test, y_pred_after)\n",
    "\n",
    "assert r2_before < r2_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fb770",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "650b24c3f3b868c6f8ea6835b063f429",
     "grade": false,
     "grade_id": "cell-aa9f5444c49aa249",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = l2.forward(l1.forward(X_test))\n",
    "\n",
    "r2_lumo_nn = r2_score(y_test[:, 0], y_pred[:, 0])\n",
    "r2_homo_nn = r2_score(y_test[:, 1], y_pred[:, 1])\n",
    "print(f'R2 LUMO: {r2_lumo_nn}\\nR2 HOMO: {r2_homo_nn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4037a8e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "769d31191675e9386f0c2cf068095f59",
     "grade": true,
     "grade_id": "Bonus_Beating_Ridge-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Bonus Beating Ridge - 1 point\n",
    "\n",
    "print(f'\\tRidge\\t\\t\\tNN\\nLUMO\\t{r2_lumo_ridge}\\t{r2_lumo_nn}\\nHOMO\\t{r2_homo_ridge}\\t{r2_homo_nn}')\n",
    "\n",
    "assert r2_lumo_nn > r2_lumo_ridge and r2_homo_nn > r2_homo_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4e2bd5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba0d17f177b2dd43f1e8a233e262d2c7",
     "grade": false,
     "grade_id": "cell-1d228c998d797378",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Quite possibly you were able to beat the ridge regression at that point, only with one hidden ReLu layer. Of course the hyperparameters of both models haven't been optimized yet, so you can get even better. But please do this in a separate notebook ;)\n",
    "\n",
    "Let's have a look at the final plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c753d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "327ca0e965f5286f79ad47af9ac2d9a4",
     "grade": false,
     "grade_id": "cell-afdaac610e54e883",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axs[0].scatter(y_test[:, 0], y_pred[:, 0], alpha=0.2, s=5)\n",
    "axs[0].plot([-6, -1], [-6, -1], 'k')\n",
    "axs[0].set_title(f'R2 LUMO: {r2_lumo_nn}')\n",
    "axs[0].set_xlabel('true LUMO')\n",
    "axs[0].set_ylabel('predicted LUMO')\n",
    "axs[0].set_xlim([-6, -1])\n",
    "axs[0].set_ylim([-6, -1])\n",
    "axs[1].scatter(y_test[:, 1], y_pred[:, 1], alpha=0.2, s=5)\n",
    "axs[1].plot([-9, -3], [-9, -3], 'k')\n",
    "axs[1].set_title(f'R2 HOMO: {r2_homo_nn}')\n",
    "axs[1].set_xlabel('true HOMO')\n",
    "axs[1].set_ylabel('predicted HOMO')\n",
    "axs[1].set_xlim([-9, -3])\n",
    "axs[1].set_ylim([-9, -3])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2655050",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fbdcb385dc9820f3017fac00e1b0e90",
     "grade": false,
     "grade_id": "cell-1e05cf72bc62162b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**You finished the Exercise!**\n",
    "\n",
    "Next time we will start using Tensorflow and Keras to build neural networks.  \n",
    "\n",
    "Here is a little example for our application. Be aware that Tensorflow does things slightly different internally, so it might give results that differ from your own implementation.\n",
    "\n",
    "You can use this example as a start to experiment with own implementations **in another notebook**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42869c0e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14d7e5ef4dc6529a3b70adc19b5c24c3",
     "grade": false,
     "grade_id": "cell-b41ed4cb02d0788f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "n_hidden = 64\n",
    "lr = 0.05\n",
    "# A low number of epochs so the nb doesn't slow down during grading\n",
    "n_epochs = 5\n",
    "batch_size = 100\n",
    "\n",
    "inputs = keras.Input(shape=(X.shape[1],))\n",
    "hidden1 = layers.Dense(n_hidden, activation='relu')(inputs)\n",
    "outputs = layers.Dense(2, activation='linear')(hidden1)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"simple_ff\")\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.SGD(),\n",
    "    metrics=[\"MSE\"],\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "r2_lumo_keras = r2_score(y_test[:, 0], y_pred[:, 0])\n",
    "r2_homo_keras = r2_score(y_test[:, 1], y_pred[:, 1])\n",
    "print(f'R2 LUMO: {r2_lumo_keras}\\nR2 HOMO: {r2_homo_keras}')\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axs[0].scatter(y_test[:, 0], y_pred[:, 0], alpha=0.2, s=5)\n",
    "axs[0].plot([-6, -1], [-6, -1], 'k')\n",
    "axs[0].set_title(f'R2 LUMO: {r2_lumo_keras}')\n",
    "axs[0].set_xlabel('true LUMO')\n",
    "axs[0].set_ylabel('predicted LUMO')\n",
    "axs[0].set_xlim([-6, -1])\n",
    "axs[0].set_ylim([-6, -1])\n",
    "axs[1].scatter(y_test[:, 1], y_pred[:, 1], alpha=0.2, s=5)\n",
    "axs[1].plot([-9, -3], [-9, -3], 'k')\n",
    "axs[1].set_title(f'R2 HOMO: {r2_homo_keras}')\n",
    "axs[1].set_xlabel('true HOMO')\n",
    "axs[1].set_ylabel('predicted HOMO')\n",
    "axs[1].set_xlim([-9, -3])\n",
    "axs[1].set_ylim([-9, -3])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
